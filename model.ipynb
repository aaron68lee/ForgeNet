{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-type Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(training_dataset) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39m# Generate random noise and text input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         noise \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(size\u001b[39m=\u001b[39m(batch_size, latent_dim))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         text_input \u001b[39m=\u001b[39m get_random_text_input(batch_size)  \u001b[39m# Custom function to get text input\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Step 1: Data collection - Prepare your labeled dataset\n",
    "\n",
    "# Step 2: Preprocessing - Perform necessary image preprocessing steps\n",
    "\n",
    "# Step 3: Image-to-text conversion - Use an OCR library to extract text from images\n",
    "\n",
    "# Step 4: Model selection\n",
    "latent_dim = 100\n",
    "text_dim = 128\n",
    "image_dim = 28 \n",
    "\n",
    "# Generator model\n",
    "generator_input = layers.Input(shape=(latent_dim + text_dim,))\n",
    "x = layers.Dense(256)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(1024)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "generator_output = layers.Dense(image_dim, activation='tanh')(x)\n",
    "generator = tf.keras.Model(generator_input, generator_output)\n",
    "\n",
    "# Discriminator model\n",
    "discriminator_input = layers.Input(shape=(image_dim,))\n",
    "x = layers.Dense(512)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "discriminator = tf.keras.Model(discriminator_input, discriminator_output)\n",
    "\n",
    "# Combined model\n",
    "gan_input = layers.Input(shape=(latent_dim + text_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "discriminator.trainable = False\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n",
    "# Step 5: Data preparation\n",
    "# Prepare your training dataset with input text and corresponding preprocessed images\n",
    "\n",
    "# Step 6: Model training\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(len(training_dataset) // batch_size):\n",
    "        # Generate random noise and text input\n",
    "        noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "        text_input = get_random_text_input(batch_size)  # Custom function to get text input\n",
    "        \n",
    "        # Generate images from noise and text input\n",
    "        generated_images = generator.predict([noise, text_input])\n",
    "\n",
    "        # Combine real and generated images for the discriminator\n",
    "        real_images = get_real_images(batch_size)  # Custom function to get real images\n",
    "        combined_images = np.concatenate([real_images, generated_images])\n",
    "\n",
    "        # Labels for real and generated images\n",
    "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "        labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(combined_images)\n",
    "            discriminator_loss = loss(labels, predictions)\n",
    "        grads = tape.gradient(discriminator_loss, discriminator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "        # Train the generator (via the gan model)\n",
    "        noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "        text_input = get_random_text_input(batch_size)  # Custom function to get text input\n",
    "        labels = np.ones((batch_size, 1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = generator([noise, text_input])\n",
    "            predictions = discriminator(generated_images)\n",
    "            generator_loss = loss(labels, predictions)\n",
    "        grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "# Step 7: Model evaluation - Assess the performance of the trained model\n",
    "\n",
    "# Step 8: Model fine-tuning - Fine-tune the model if necessary\n",
    "\n",
    "# Step 9: Generation - Generate new images based on input text\n",
    "input_text = \"Hello, World!\"\n",
    "noise = np.random.normal(size=(1, latent_dim))\n",
    "text_input = preprocess_text(input_text)  # Custom function to preprocess input text\n",
    "generated_image = generator.predict([noise, text_input])\n",
    "# Display or save the generated image\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-type Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128  # Dimensionality of the latent space\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# Variational Autoencoder\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "vae = VAE(encoder, decoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(inputs, outputs):\n",
    "    reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "    reconstruction_loss *= 28 * 28  # Image size\n",
    "    return reconstruction_loss\n",
    "\n",
    "def kl_divergence_loss(z_mean, z_log_var):\n",
    "    # Kullback-Leiber Divergence Loss\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return kl_loss\n",
    "\n",
    "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
    "    reconstruction = reconstruction_loss(inputs, outputs)\n",
    "    kl_divergence = kl_divergence_loss(z_mean, z_log_var)\n",
    "    return reconstruction + kl_divergence\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "vae.compile(optimizer, loss=vae_loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(dataset, epochs=10, batch_size=64)\n",
    "\n",
    "random_latent_vectors = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "generated_images = decoder(random_latent_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_model = '/content/scrabble-gan/res/out/big_ac_gan/model/generator_' + str(epochs)\n",
    "\n",
    "# number of samples to generate\n",
    "n_samples = 10\n",
    "# your sample string\n",
    "sample_string = 'machinelearning'\n",
    "\n",
    "# load trained model\n",
    "imported_model = tf.saved_model.load(path_to_saved_model)\n",
    "\n",
    "# inference loop\n",
    "for idx in range(1):\n",
    "  fake_labels = []\n",
    "  words = [sample_string] * 10\n",
    "  noise = tf.random.normal([n_samples, latent_dim])\n",
    "  \n",
    "  # encode words\n",
    "  for word in words:\n",
    "    fake_labels.append([char_vec.index(char) for char in word])\n",
    "  fake_labels = np.array(fake_labels, np.int32)\n",
    "\n",
    "  # run inference process\n",
    "  predictions = imported_model([noise, fake_labels], training=False)\n",
    "  # transform values into range [0, 1]\n",
    "  predictions = (predictions + 1) / 2.0\n",
    "\n",
    "  # plot results\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(10, 1, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    # plt.text(0, -1, \"\".join([char_vec[label] for label in fake_labels[i]]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "year = 1947\n",
    "print(2023 - year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.800</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12700</td>\n",
       "      <td>2850</td>\n",
       "      <td>50.825</td>\n",
       "      <td>2.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25400</td>\n",
       "      <td>5710</td>\n",
       "      <td>50.851</td>\n",
       "      <td>2.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38100</td>\n",
       "      <td>8560</td>\n",
       "      <td>50.876</td>\n",
       "      <td>2.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50800</td>\n",
       "      <td>11400</td>\n",
       "      <td>50.902</td>\n",
       "      <td>2.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76200</td>\n",
       "      <td>17100</td>\n",
       "      <td>50.952</td>\n",
       "      <td>2.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>89100</td>\n",
       "      <td>20000</td>\n",
       "      <td>51.003</td>\n",
       "      <td>2.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92700</td>\n",
       "      <td>20800</td>\n",
       "      <td>51.054</td>\n",
       "      <td>2.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>102500</td>\n",
       "      <td>23000</td>\n",
       "      <td>51.181</td>\n",
       "      <td>2.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>107800</td>\n",
       "      <td>24200</td>\n",
       "      <td>51.308</td>\n",
       "      <td>2.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>119400</td>\n",
       "      <td>26800</td>\n",
       "      <td>51.562</td>\n",
       "      <td>2.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>128300</td>\n",
       "      <td>28800</td>\n",
       "      <td>51.816</td>\n",
       "      <td>2.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>149700</td>\n",
       "      <td>33650</td>\n",
       "      <td>52.832</td>\n",
       "      <td>2.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>159000</td>\n",
       "      <td>35750</td>\n",
       "      <td>53.848</td>\n",
       "      <td>2.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>160400</td>\n",
       "      <td>36000</td>\n",
       "      <td>54.356</td>\n",
       "      <td>2.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>159500</td>\n",
       "      <td>35850</td>\n",
       "      <td>54.864</td>\n",
       "      <td>2.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>151500</td>\n",
       "      <td>34050</td>\n",
       "      <td>55.880</td>\n",
       "      <td>2.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>124700</td>\n",
       "      <td>28000</td>\n",
       "      <td>56.642</td>\n",
       "      <td>2.230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2      3\n",
       "0        0      0  50.800  2.000\n",
       "1    12700   2850  50.825  2.001\n",
       "2    25400   5710  50.851  2.002\n",
       "3    38100   8560  50.876  2.003\n",
       "4    50800  11400  50.902  2.004\n",
       "5    76200  17100  50.952  2.006\n",
       "6    89100  20000  51.003  2.008\n",
       "7    92700  20800  51.054  2.010\n",
       "8   102500  23000  51.181  2.015\n",
       "9   107800  24200  51.308  2.020\n",
       "10  119400  26800  51.562  2.030\n",
       "11  128300  28800  51.816  2.040\n",
       "12  149700  33650  52.832  2.080\n",
       "13  159000  35750  53.848  2.120\n",
       "14  160400  36000  54.356  2.140\n",
       "15  159500  35850  54.864  2.160\n",
       "16  151500  34050  55.880  2.200\n",
       "17  124700  28000  56.642  2.230"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N       lbf       mm      in.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    (0, 0, 50.800, 2.000),\n",
    "    (12700, 2850, 50.825, 2.001),\n",
    "    (25400, 5710, 50.851, 2.002),\n",
    "    (38100, 8560, 50.876, 2.003),\n",
    "    (50800, 11400, 50.902, 2.004),\n",
    "    (76200, 17100, 50.952, 2.006),\n",
    "    (89100, 20000, 51.003, 2.008),\n",
    "    (92700, 20800, 51.054, 2.010),\n",
    "    (102500, 23000, 51.181, 2.015),\n",
    "    (107800, 24200, 51.308, 2.020),\n",
    "    (119400, 26800, 51.562, 2.030),\n",
    "    (128300, 28800, 51.816, 2.040),\n",
    "    (149700, 33650, 52.832, 2.080),\n",
    "    (159000, 35750, 53.848, 2.120),\n",
    "    (160400, 36000, 54.356, 2.140),\n",
    "    (159500, 35850, 54.864, 2.160),\n",
    "    (151500, 34050, 55.880, 2.200),\n",
    "    (124700, 28000, 56.642, 2.230)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('data.csv', index=[\"N\", \"Lbf\", \"mm\", \"in.\"])\n",
    "\n",
    "print(\"N       lbf       mm      in.\")\n",
    "for row in data:\n",
    "    n, lbf, mm, inch = row\n",
    "    #print(row[0])\n",
    "    #print(f\"{load:<7} {length:<8} {n:<8.3f} {lbf:<8.3f} {mm:<7.3f} {inch:<6.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
