{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Set\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "def getUniqueSetOfChars(words_lst: List[str]) -> List[str]:\n",
    "    '''\n",
    "    Find a list of unique chars in words_lst. This is needed for the one hot encoding to turn words into numerical vectors\n",
    "    '''\n",
    "    unique_chars = set()\n",
    "    for word in words_lst:\n",
    "        unique_chars.update(set(word))\n",
    "    return list(unique_chars)\n",
    "\n",
    "# print(getUniqueSetOfChars(words_lst))\n",
    "\n",
    "\n",
    "\n",
    "def oneHotEncodeStrings(unique_chars: List[str], strings: List[List[str]]) -> List[List[int]]:\n",
    "    char_indices = {char: i for i, char in enumerate(unique_chars)}\n",
    "    encoded_vectors = []\n",
    "    \n",
    "    for string_list in strings:\n",
    "        encoded_string = [\n",
    "            [1 if char in string else 0 for char in unique_chars]\n",
    "            for string in string_list\n",
    "        ]\n",
    "        encoded_vectors.append([val for sublist in encoded_string for val in sublist])\n",
    "    \n",
    "    return encoded_vectors\n",
    "\n",
    "\n",
    "\n",
    "test = ['apple', 'banana', 'orange']\n",
    "print(oneHotEncodeStrings(getUniqueSetOfChars(test) ,test))\n",
    "# print(len(oneHotEncodingCharacters(test)[0]))\n",
    "print(len(set(\"\".join(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew_process(image):\n",
    "\t# deskew image\n",
    "\timage = ioski.imread(sample_img)\n",
    "\tgrayscale = rgb2gray(image)\n",
    "\tangle = determine_skew(grayscale)\n",
    "\trotated = rotate(image, angle, resize=True) * 255\n",
    "\treturn rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_index(dictionary, key):\n",
    "    keys_list = list(dictionary.keys())\n",
    "    try:\n",
    "        index = keys_list.index(key)\n",
    "        return index\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # load variables.pkl\n",
    "import cv2 # image preprocessing\n",
    "import os\n",
    "\n",
    "IMAGE_DIR = os.getcwd() + \"/Datasets/words/\"\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_HEIGHT = 128\n",
    "\n",
    "# Step 1: Data collection - Prepare your labeled dataset\n",
    "with open(\"./variables.pkl\", \"rb\") as file:\n",
    "    (\n",
    "        filepaths_lst,\n",
    "        words_lst,\n",
    "        filepaths_dic,\n",
    "        words_dic,\n",
    "    ) = pickle.load(file)\n",
    "\n",
    "with open(\"./filtered_data/processed_images.pkl\", \"rb\") as file:\n",
    "    (\n",
    "        processed_images\n",
    "    ) = pickle.load(file)\n",
    "\n",
    "\n",
    "print(\"Load Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(image, TARGET_HEIGHT=100, TARGET_WIDTH=100):\n",
    "    \"\"\"This function will pre-process a image with: cv2 & deskew\n",
    "    so it can be process by tesseract\"\"\"\n",
    "    '''\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.resize(img, None, fx=.3, fy=.3) #resize using percentage\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #change color format from BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #format image to gray scale\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 5, 11) #to remove background\n",
    "    '''\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    # print(original_height, original_width)\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    if aspect_ratio >= 1.0:  # Landscape image\n",
    "        new_width = TARGET_WIDTH\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:  # Portrait image\n",
    "        new_height = TARGET_HEIGHT\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image to the target size.\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Pad the resized image to make it the target size.\n",
    "    padded_image = np.ones((TARGET_HEIGHT, TARGET_WIDTH), dtype=np.uint8) * 255\n",
    "    x_offset = (TARGET_WIDTH - new_width) // 2\n",
    "    y_offset = (TARGET_HEIGHT - new_height) // 2\n",
    "    padded_image[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image_skew(image, TARGET_HEIGHT=100, TARGET_WIDTH=100):\n",
    "    '''\n",
    "    old preprocessing function that skews the original image\n",
    "    '''\n",
    "    #print(image_path)\n",
    "    height, width = image.shape\n",
    "    #print(f\"Image dimensions: {width} x {height}\")\n",
    "    image = cv2.resize(image, (TARGET_WIDTH, TARGET_HEIGHT))  # Resize the image to a fixed size\n",
    "    image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image_tensor = image_tensor / 127.5 - 1.0  # Normalize pixel values between 0 and 1\n",
    "\n",
    "    # Add the preprocessed image and its corresponding label to the lists\n",
    "    processed_images.append(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@693.455] global loadsave.cpp:248 findDecoder imread_('/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/Datasets/words/a01/a01-117/a01-117-05-02.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@911.755] global loadsave.cpp:248 findDecoder imread_('/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/Datasets/words/r06/r06-022/r06-022-03-05.png'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_images = []\n",
    "bad = []\n",
    "TARGET_HEIGHT, TARGET_WIDTH = 100, 100\n",
    "\n",
    "# TEST_SIZE = 10\n",
    "\n",
    "# Step 3: process the images into normlized 100*100 grayscale images\n",
    "for i, image_path in enumerate(filepaths_dic.keys()):\n",
    "    # if i > TEST_SIZE: \n",
    "    #     break\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    #if image_path in bad_data: # bad data\n",
    "        #continue\n",
    "    image = cv2.imread(IMAGE_DIR+image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        bad.append(image_path)\n",
    "        continue\n",
    "    height, width = image.shape\n",
    "    \"\"\"\n",
    "    change preprocess methods here\n",
    "    \"\"\"\n",
    "    padded_image = pre_process_image(image, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "    # Add the preprocessed image and its corresponding label to the lists\n",
    "    processed_images.append(padded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(bad))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bad' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Bad\", bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ec74fbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test processed_image by plot out one processed image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(words_lst[50])\n",
    "plt.imshow(processed_images[50], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 115239\n",
      "Train set size: 92191\n",
      "Test set size: 11524\n",
      "Validation set size: 11524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Data preparation\n",
    "# Prepare your training dataset with input text and corresponding preprocessed images\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Variable \"processed_images_to_text\" holds pairs of images (tensor arrays) to their text label\n",
    "# Ex. [tensor array for \"Exchange\", 'Exchange']\n",
    "processed_images_to_text = []\n",
    "for i in range(len(processed_images)):\n",
    "    processed_images_to_text.append([processed_images[i], words_lst[i]])\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train, test = train_test_split(processed_images_to_text, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the test set further into test and validation sets (50% test, 50% validation)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(\"Dataset Size:\", len(processed_images))\n",
    "print(\"Train set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))\n",
    "print(\"Validation set size:\", len(val))\n",
    "\n",
    "image_labels = [train[i][1] for i in range(len(train))] #List of all the text labels after shuffling \n",
    "images = [train[i][0] for i in range(len(train))] #List of all the image tensors after shuffling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-base-handwritten\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import Levenshtein\n",
    "\n",
    "def cer(reference, hypothesis):\n",
    "    # get character error rate\n",
    "    ref_length = len(reference)\n",
    "    hyp_length = len(hypothesis)\n",
    "    distance = [[0] * (hyp_length + 1) for _ in range(ref_length + 1)]\n",
    "\n",
    "    for i in range(ref_length + 1):\n",
    "        distance[i][0] = i\n",
    "    for j in range(hyp_length + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    for i in range(1, ref_length + 1):\n",
    "        for j in range(1, hyp_length + 1):\n",
    "            cost = 0 if reference[i - 1] == hypothesis[j - 1] else 1\n",
    "            distance[i][j] = min(\n",
    "                distance[i - 1][j] + 1,\n",
    "                distance[i][j - 1] + 1,\n",
    "                distance[i - 1][j - 1] + cost\n",
    "            )\n",
    "\n",
    "    cer = distance[ref_length][hyp_length] / max(ref_length, hyp_length)\n",
    "    return cer\n",
    "\n",
    "def edit_distance(ground_truth, hypothesis):\n",
    "    edit_dist = editdistance.eval(ground_truth, hypothesis)\n",
    "    return edit_dist\n",
    "\n",
    "def levenshtein_distance(ground_truth, hypothesis):\n",
    "    levenshtein_dist = Levenshtein.distance(ground_truth, hypothesis)\n",
    "    return levenshtein_dist\n",
    "\n",
    "def get_accuracy_metrics(ground_truth, hypothesis):\n",
    "    return cer(ground_truth, hypothesis), wer(ground_truth, hypothesis), edit_distance(ground_truth, hypothesis)\n",
    "\n",
    "def OCR(image_path, model=pipe): # can define custom OCR pretrained model to use #, model_path=\"./SimpleHTR-master/model\"):\n",
    "\n",
    "    '''\n",
    "    # Load the HTR-DeepWriting model\n",
    "    char_list_path = os.path.join(model_path, 'charList.txt')\n",
    "    model = Model(open(char_list_path).read(), decoderType=DecoderType.BestPath, mustRestore=True)\n",
    "\n",
    "    # Perform word segmentation\n",
    "    word_images = word_segmentation(image)\n",
    "\n",
    "    # Recognize handwriting for each word image\n",
    "    recognized_text = []\n",
    "    for word_img in word_images:\n",
    "        word_text = model.inferWord(word_img)\n",
    "        recognized_text.append(word_text)\n",
    "\n",
    "    # Join recognized words into a single string\n",
    "    final_text = ' '.join(recognized_text)\n",
    "\n",
    "    print(\"Recognized Text:\")\n",
    "    print(final_text)\n",
    "    '''\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    result = model(image_path)\n",
    "    text = result[0][\"generated_text\"]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: coloured\n",
      "./Datasets/words/e04/e04-127/e04-127-04-05.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "sample = 'e04/e04-127/e04-127-04-05.png'\n",
    "image_path = os.path.join('./Datasets/words', sample)\n",
    "\n",
    "ground_truth = filepaths_dic[sample] # words_lst[i]\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAABMCAAAAACpeB9fAAAkdElEQVR4nMW8WZMk15UmdrZ73SMi96WyKmtBoaqAKuw7QJAgCbKbM+zWmGZGetGD/poe9SAzmcxGYyO1bMgZiQ12swmA2LdC7UtmVu4ZGYu737PoARoCJuuxKgMeeF/D/LPz2fWzfed4YMBf6rQtueXUgUkoi7oQkAdyNJooHDgcQQAdgxzQMrkzAoCWhOxqgZpzG6K1O0O4EXmXGCHcBQEQwj0SmrXbX6zckb8YVYBCXU4Gkl1r65J7cCEEpYRdFUjhEeQOaBWzex1dgYooAARQNSA4ZeQAAmQlxEwQTuQMwKQayITZVXC6+x8PLt3Cv9y97kvdUHQZHCnMUriQOAK4cQES6AgJ2N0sJQQrrowuzECO4R4OwuBOCEEuYASEDq4ojiDFgxjB1Lz5+Nr1M8f5L3ivNVhqKINCCGBliOjGitwlEGQszOyBrCyGYF0w5+IGgQwOFJ7Q23DoU0mOYCWEsU0s4OgJCBEN3aD98tMHO2fn6tm/IFcCigGo1YBKBmwOLIillQAOj8LgDOFQMgNyEEDLVpmgsyNCRAiWBFC85SCFAMSqI3RLytAxOjBIe+1/2l88tfrE5Pj7c50EBbJ5wkJugezGBpK9HPy7g5+sPfNQAAYHdEJ0Sg4QJQtAECdFpM6hAECbxLKyOXrucoAzMSkwAUBl5pQkAMWgoHBxL5UUcbYCKTw5EYz3Pk5vjiob769+f64CFliAHSIwq5NgC1Lcjz7+dPUGPRwg3BwF2DDcwjCzGmBiFyTMHYiDJsWEhIxRJJlzBgREDAgkR0FPoUABzs5qDORBbU2t1F4hsMf4qz/cvnzundUrD7D//bmiG4sSoCIwipaMVYHw9ot3+hdnHjwcwKMAOzmhIbsTBxM7YQAiOAkZMXCXgcHQjcOdXRRQwh0iEAMhwAxS0mBEJkxhkENDwBOYwaR7cGfpxY0nftJtzXXfn2s4GBNBSBAGSWOAosTTu/zk4uhRAAoyMXoJBGi0xxGhEETeEUTyQKQC0EICDKgcMnArjGHgJikcTMAhUYvTTIiImrCABweiUkce5ry0/NLM4mv922nxB8WmBOY5QCMFGFVhYAjT/erSAsL6wx8n9ggNBkYCsBSIERhoLQs5RiC2zaDvEOjUajCh50AAEyECwEhBCIY9SggQbgnVvfM+A1DD7sV5CudO+DOzhVbnF34AVwxDMlIQY0Myoyh32vze7Akb5RMPfz7EzYM6zzithbnzohlZE3lYEEBXY6oLglbYAqJqzhAAIYQBGEigGOZEihColjAQuW6FzIugWmT0W2OWuvUu0+D7c50yQ4pA9uBQUEpBOju4Oi4r98dr/HAAE2LtABJjRjQtCRGiYwfWgKTASvPOGBkCA0GEIkiZFIEgAoLFDSEcnAKFGICgeE8xjCHUqJlMc/qoWpL9Oysn6u/PtYRFZAokUxFDAoRqfTfPx63P+HT9cADSIAkuzCFm7H0DZncP9AQAYcAcGIHqQAAMTGYsoBYCGBRhABEYoQyEZpYBMBCs0hRRUFJBWXpv79dI8+elSt+fax1gAYYYYE7EqOBQ9g4WjmV2oa0eDqCE5BE5SkiUYCTAQowBUJCKIIkjmIl4sZQoiFACwo0KokB4hFLCsGwE7qCW2IEiIUQgkbU63nn/+uuXZrhdNcQfEIdJIgAckWoMsJDA0MO9tZXJ8mDxEXCZQR2UBTFEs4U7EpEFkooiVq5umRkUE2lJ5EZB4JYRgiEQKZTMhZk7xzoAwalqIRlSBCF1w63Pbs/Mz+ThF2lxhh4h5/9XTqZgQeYoDkDEFObTyQi53Whw4REAKAAoJUECgGwUANFCiKCCEzC0XAdAQJhD7rGTQGhhcS9tZ6EBDgoY1pp3Vhw5AqJKGqGGNXm5c+u6nn+hDq1SL8X3v9e2MgIEoAgACArHdrhrg80Yg597BFzqJEEgFAEPREAUEjTBEpWJNoDhBFHQnCEFMARCEHJwqGoFYAaJtAlWrikUIxSCE2OnFbTR7e9t12f/atkCx3OVwQ+omyyBBhIGBEIoozVDmNm/2T9q1uQR3peSAJwKAQZwEAZ6VwF2yOJihgwAUoJUgSYVMUcEcmiQS6RAZSwqjUgkowAwMkAxi4CoiLw92Ny+Oz49izqJi9aj8v25cqfIguGA5hY85e5gd3Vpq5tMyxkqjwAQ5IUcWBHCg4y7AIROqE2EaowIlBSESknjnDWE1YkklAk4oqVIHbp0XUakQhYZDFwsgwJy2r21fW/mbOXNF7NLg4S978+VomSIsECiMr5xahajRFMf1/cseeoeDuAEJMhgFOGKZIUq0o6r6MADihBqEIM7UZUF3CEICN2YHN0CzS1FG0EQ0Dh4RUbABFpqH+pk7+gePncl506H1ZK4/YA+B9ERTBIA5PLlSUl0qoujycE+n6y6R5A7BII4IAjY0EnB6jAQd0eLSKkViggnDMMIDyRFBeCS0NDNCAQ0h4QQQqvGgsgY7kCpK97cuHntIF+Zxykczs8jwQ/g2nLfDIgwHHD+35J56KHd3rASc2vxCPnV2IPIXdSRKjETMoiESgmVNXxKgm2CSogA1Zim/UTFg0ogoQAYM4ijq6EhIRp6ALkBoULp4qg7cQYwxaUSLJ3/gLopFRVgio6NBRC09VN716b3yokn5wf4CFzdCcGgEDkkd4/E4I6CYQQQtTopI7oHAoLmqMMiAQcGBrmPe6waRYS6lKRAGBFgkzHcqexub00Xn1pE8HGXKUb+A/QmRqw8QJEcnVODrVG+uiMwt/LqDPrDAcQq14iM7qSlhxHOCECGFNApCYFlN0YiV0RqqlSgrckQlYOjJIAcIRScGIlLQkb3HAER2G3cvZXpdMZyfAwwYP0h+RUBApFULTNqAayLw17X6dLfnAV9lBolSsKwCCfDZCIG3yRIC3fKpBIEAGDkSIa1QUHHpsdGooiQEnmL6LmAMASJIARBcURk841bx6cuXhEZfzKTzL0C+/51kzgGIRDGfutEyZVh0qU2z59KSI8ixWIYBjsSlMk4ACA8OBECpkzk6AJOwtCGE2IbLinXCRCRCYW8RIQiOqB7tOrFomvHJkJcxneORnG40q+5gXGvX4W2P6CWcCEgcOS2mXGnAJy2mxMZ1hcWAvUR/FU9EAgIMXIQlDAJQC+IJgwNp7DACCXyzATgAdBlxKDoSEABPMSCMFmYM5YuRRddJimlxN72IdcDsfbWXMzOM1ji789Vg5Jyh7lnhSB0Krz9nyaTqE5ixKP4KxsZgRE6euqaSggpIqRh1kKcMIJTIARAlwOIrTA5GoYJxLgbUGDiQlws2CILY1QgNYSgH3x6vQyee7lnim11dJo7FvkB/oribSIA7IGjqwm2Vo4Hrz8jJeQR7jUIpBNGo1DmYDLoiC36JdSJO6YcRuAlyFtIpRMJIEADBIr/z5/9m+6oaHIIgDrYyLy0d74ekz82n6aT3jytIhCjfn+uCcF7bWIriVGt6sfwujMMLqwkoUepEdHNmKEAYVJwwggmDymlYsuBHgXBUYmgEIJFZQSAyCWASQnaHMASXgSo9ggjxxYIEpSDL/epf+m8tztbK1Vmo7D0A2oJ8+QpWuZAA3MKGD8Yj0nnxYEfJTR1yEAA3DU9CBwoUwCiGOYCNRU0CgbAb9RfL70a2MhdKDkU5kjJWLsKiyOLpUAuGesmzHn8+fUG4swS+AAfLA7mzNDlB+jDYB5RNZoBCgp61x3suZfTS5FC+VHyq3smgq4lLX0sYFKmIsTUMEqxSpAU0I1cIzIzAAgggAUAUlBhgJTQs3kIQwdUuVvSAD366pC0epyURuX46LWCXsUP6XOQDTGIAlslcGv2/6hdR+sL3Nb0KHoHkxAEVckhk5NDIQEk88owUMihkCEjGDkjBEagMwYAATi5AzkCKaOGiRmqJXRws+bmnYn3L1+sy/ZB1z0112MLLywtgDmwhxSFiGj3v36weOPw7b99mKkq4Uq1emDtgYj71w8O0pnnJdAiPYKOCEYaZCCeXIMJoIsIRUUKAO84XJkDCyZD4A45rE2eORzQg6UN6TCxlxAKQmdRZi0dTvcOj/LK28vkY+OlEzOmSFlAOkIPD1ZPgWblw3+4Py1xYvfhllrtDVkwZWAIjMNq1EyeeByJjOJR5jkJjENLrSgRgaEcYexOgOQJyQIBShgFlEoVxbEKRm8EwZWQ2nH0Sli2Fi2Dc0MsZqnsbY0BT54A6/pTuTCwCHAPFEFEBycOD3IYfXwfoi17D+8/FRunfkE3A0Qqwzsbx5RPz9fCwWEP50oURCoYidWQ0AQM2B1NA8TZXVTRUTqPmlS8AJcEHlncWckpMTI5EzqiYRSCcADf/u3Hnaz9N6uyc/V4+dR8mAOEIEkEAnh4KLDq8ca5ttnxhcvPPtRUzs5e3I2kpILDrz4ZNr31l3tohCaPEIinwugcGcw7gEjgTBCBaXpUeln8mxyEEnI8h5ECDIDc4Zu5KpA7EIc6W0CKQm0BsQry8fXb4+BLj/Wm+8fTzaf6pSPNAR2JAQqyGwKV4w/v1bjSp/mXX5p5qKkR5B0SVBEZpLn3zrHB4Ndn+ZvZwiNpMJo0AjNA39oohgSIqTna/3o4e3m5AiEy4sCYl0BzZAgkbilC6wAHQJi2/TrUCZkaGriRW7P90bDjc2+nZjpKRzMzlKYgQJBRnChEzUWKf/Z3vqr9dvmt9fzwukeaPg/QAswy6tF14qZ66hn2IA5+lJ5iSj0CCAskAIEOA82lTG7+fi+mo1/WnAxJQ5NElwPZmMisq0gBWwEJVeR+jQashcCEIKC0e58/0Dz3+uPUPTgc9i6yQt1RIg+QUCXualb1vfugo7255362IkkfaqprhCum4jhFvfvxMeeZN/oMIIT2KCsnFaTImtQ8F0PM0UakZnjrD1vk27f3+qVUY66NwDAbAWYMcE0ZsaGAFAWSYoIOHFMJJLTg0MnW9VGTfvl2huFhVdYvuGOqAttvrpSLJAuy4fvv7nFz+c31ZUJ8+L1wT1kKpaoE+fQuNW1ZOlGhaWKHRyglQKJjNHXkifAkA5KTjf5487ovnZ9sThHZxB1UmZQBIULdLENQ3zr0IOtc2cED1J0YiTrT4y/uHPj6E3XT7h7fPX2ij0oqZEROIsRGoViOPvjNsVUrvz7PYfQI6VFYgftdaAcx3d6b+uj03y4jW03UPQpV4GgViFwEjfrQaqLh4Zd/LCcW3+rNTlcc3ZOUVHcQoFQwAVBEGBZE0QxmSATfjOSABaBhdb2/OZFJI8X3N13OrSsREjtxoErtgKFR2k/+7igxv7gGwqiPEFqgQSTncPLQOweNSe/x1FYsZMz8cB+AaUpdUGFoJVgLVR6jz/60S+f/urf9YKmwaeaoyMh63qY6OiD2jA6WpEiBCMIcBkEWjO4Ihrr/3tYRrjyzCgc7w+nzp8CRnAzZFUEmVNVttJOvfluYZtZe6AuE1N1kM7vX5GBkWGB0a/fSchpPZG31v5g6gkQqrhE+2rjV1Xvl3CAnY3SygEfp1QHabBQWXVhLCW14/eqDav3VU3pn7CNF4m9qQ28jGMyyRhgwk5UuGwJoRWGOykAQDslQpjBiSz+aPb7a8uxqPeMuAOyG7iBddvcCG//+EPrzp35yCowZzJNFgGGEoh6MJ9dqtGtfP4izb/+Zq2Rto08QzP7+l914svSjXEsDKgU4HsEJKmvBhQHbCBfycnDjwXZ58Vfz7HUPEMALorEWBmYLhfa4x9nI0AE6FEqmaIFuyB6MxqnsXt0Ztfnygj+Y3Fp6bblWSOSqFXQIILVHKe3GO8OKeid/tdYXNQAowJ4cC7Xmo/9zlqvT0xu/uz27gh9d+i+mJg0W8ohRef/uzOaQf3WpQqwAnQEfRQvX8mC8vKYc1DJZ6N4XG/sLg7cWE+hk7uJCV7OGY/EGK0ZQ6Ko6JVQLYKUEGG5q0U6qnidjyq1GuXF110NeqTZuHcLzZxMGCKcoCoTiEqDhG//bdaS12edW+1E6ImzRCUwCOmkOP766sDbz2cb9L5AnB+2fTXUTL0DYTa99gN6mep0xwg2FilppCgNTxw5WY5RufNhf6aFZxiAyT93Eh/tby2aC4ZKiO7q6+5Ut/vyEZKPSKFBphsOpLO7dz6dOLVKlPYIUJACMuQsLDvIy/M+f/Oz5PpjUOdQOr+8egZycv3fQtS+f6xGQOHeZzABJJMKo/fBmPe6f+vHFAbMFKyBWrKDBMd3/zUe2s91+1QaffP0gLX57L2Rsjo7TT/eNut4vLtmEUSroKsduNE7YnU6pKEsB7e5ZzBKaMLQVQHBWubV1vJK7HpY2CZS9z7YbHTz9BHuH4/sns/Dx1//rXZ+50lHv7tzfzhEBc/EwxMIESGHqzdZ/eBevHlwih3OzCdqdOxvjqvfm8d7ni+dfnEnBgdyaioN5NkHF8sW7adrNz59KqhUlQEhWAoBKxME/vHuc5pqNqeKJ//6J3840f6ZqQJTbzsr2vW6MsfBqv+UAayim0d4fHp9Q2slUavYGRp/4VBaIzQ16ombRtIfJDh4jsfDZAN3/ZGf7aPWnF2dKISxNuybt/f/wLqW4T6m027cuK4oSl2nOmNFRC1F3/P6XVxfPXDranB3fiycc2xu3jyLVs1fz7OqPZjmilQ7NkwYyGAhbufP/bKcW51+omThc2amBoBk3Py4fvN+k8y+cfOePO7M/f3PcG+x/627faHo0/WAvVxv00jL2IbMqRtMd3Zku7U/Hn42X5fXV0zOTr/7nruvFIpN4Dkgtm/vu4QSXK00sBN3BreHxITz5RF9Cvdk7uDBpJ+/8UROtL2G/8+Fel52xmCUmQkMIx+bwnd/tucztz9v+oR/tL8aDTyeFu0v3xvn1tYFzcIFcUnCAZYkQjOb25lw3sDcfT+6RAbnXCoAdpa7b/+T/7vpLbz0vPbvx5tuDY5Cv/8xVOmSH1O583muJVi6gKSuiRKt3dq5XDJ+UvzdanOh/99i9v7uJ0vzmFe4rlQRK3Eo7Obrx/Fx2F1Ju745ily6/UXPUg06Rt561nT9ZD+rZ12P2xt7WRuqBkHWVEmCbKjMeP/jk/vaO0nG60kvDxW4yN/mCLcXSqeO9yycGzqWWClX7IObJApLA9MY9QGmfeLZqBMUjJlF4xqZOePyHT2U6e+HigE++NHtyALS0vfdnrsyACD7a5NluY/YXZ0W0jphSGW9/tH9Ybo1HZ6th61/Q72H7M8Pwjd0FzQKB7M7Trb0RrTW9jAltem9zcmf4kxcXQZBLV2K2o2Z3g+q2i6d7xxv7UQOFWZcivCpujfnBR0d7BwJwZnX52ROHmCb9fPPL3c6hfgBPv76UCRoQFQaMNtcdWUEZb/92Q13iJ8tGzkrgHtwXSzQ9fO+DleNy+rnVOmbOL1FoC1un/sy1ZOyQuzjoNVOsziySoYN2vvXp9XIwky6dvv6bfbNmu7r5Gji5O3HloMBm0PnB7a2IlZUUNqzK3v3RZDOdXHSh1k1Sdzy/DFcBbb73/Mws47i35J3nglwQiwWqDt97l1P/uZm1H60vz5v0pwuzzY2YBM2n0cW35gOAMisAYknsmiNqkem1+wjY/vQ8IkkXOZAhhQXY3kf/0SfyxE8e7xvns1H88LNd+NG3sYksso6++lPs7wyunCasXZPDxj+9SxKHv3pxsPJbSVgv10+tbDs4pqWlYCkeCNaObu7p9IkBhIO35X7Lt+q/ulgDKCOi7aW2G/YG5crc+lsz0JR+xRaZckRSB0zuh++8U1Zt/uK/nMHFGZ+Oh74Ik70jVWQ6/+M5yYzEycCjjg44Okxs0t6cYN3PLy9RZZjDkYgQtNXhbz/GDJdePp/dRcghJofD185+W/YEgnVHfxjBPiz+bMGYI0Ecv3eDulZeemnBZt5u+vxh9Bcmm4VyVE9mTU7M5hAH9zd35p8fWIs7Ovv1Ae+PXrlcGXNQYABPxCbLT517Tk4s5RZm5hdZkiuycYK2Td3eB78f4uTEhRcWjqWPpTyYpNPcHu9OrJ5dfGOd0EFQtNSFOyKiVNwBZLrTQVP+zWrC1jUzOgcGKIw++mowczj3/LmEHmoQdPju5z96fvBnrhgSzbTdm6Cnx1eAMrI15ct3nYxe/Zslb3r/ymBydHutPx2tPH7gvV/OVRBRq4N1Wzt3F/9miUi1vrZ7h8cb/bU6BQR06EBL+92D4+mrS6eXqHKAnXpJ1NwFKIoiNfuf/eYoTp8/89IMzbJEbB0PMsLhJoxx5vSPz/QzOjs2ESUaSGCWpAkGmViWyfKTPUSDxBTJWSFgcuP3arB6cQ0RCCCBT7ZvPv4i5+/kHCipO5gUtYWfspIzgD/4J/d84qm/XibYqs5A11kaUHt4+coYzp8Np1J5oE7H46P0+vlcUlaob0ts27lL/cpMOZBDb+DMF6dO7m9cosqCprFEBEJBxgVSNLu//dJmx8uPPTVPmMoIdjdL98r8/u3dCfryG0/MJdCEjOJmKCghHpQLqGw98MPBczOsLLlDR4eAUCs71tbnz78wU0XnUEdnW39Y/Dnrt3sQJTJCe+M4fHBxaZCcBdr287sxc/aZl+fI83lSGI9n8tze7/bPbk8un2GSqAWA/PDa7aPXf9QvzmU8bqw72nzqr070v9H4crGdkfFIdv/4enQhnR0uTBVEWSy6YGiGHx9PgZdffHaGhSyX0a2vZ15djvZaM/b+Y+dmUZOkACfqQFgR2bqAcJatXinW7xHlCAFW/2aiuXd7SGrPzDC0gmTN/uHft7/o33n+2xKxDqf24HMoTT8PBHtZp5NP70paPvvSXNLokrGWYZqb/o7n9tXS8ign8inQ0firf9xbuMxBaGXUohzsLb6+yI4q2cJ5Um4fD2j4fjpZcddAG+1o6wV2s8DADuH+V8NeK2+8PEvGXaiNPx6cOCmHn2+50bO/PpE5BDsTCAeSyKxFGb0ISjN/ALFIyY0KVBEAThx6/R4lxyxQOKyJe/9LnPnJ7I1nF77TpgTqZOdQTa683SOCiU0OvtxLvXOvsiIWrankhcPF/Q+HF4bu9fIMIEaoB219eVBfOilAjjqebN7amr52ruqcSEOAoDdLZXTyw9H/+GwO0vH148HBzEl0ZA4zK9tf7h+IzLzaZ89qcbS7k/1cf/Tx73eQemsrYeTWcRJAiAjEFkAicFCp6N6EVtdDFXsBU3TmFKiTu23lzcUKkNtM7e5/2q6f4c2L8/hdjRtd7xkYv3mJmgzUjv5+58jx8mwG6rimIGy6vavTt4ZY3Y0lme+CTGx89YuvYv3nfRUFD9q+fXe8+taAIrzLXiwoM7TTrzdX1hPkArvvLVZ2yQlAwwTg+N0/tamZ/LeLME2dq9759Hh6bn73xnbCSvPFqq5dW6zMEZICuqOJOzA3KI6QTlfZakKBEkAU6H60o3xw9rK5YGhz8/+4+8qV9k8vrFT2bWxi89FXX/Nx+9haVBGu40/vdOEXzlaEkF2ROA7+8Yv6r9f/nW41i3PG3hKW7rN3JwlfXcbsheNgZ3i/qX+9xoZMLOZAMd3djvFYXprTXjfZ/t3G1tmJSGRou2jB9jZNSvfTFyokI7Sbnx76zNmN28vzO52m1QvCkbyCYolBkQEQoXSJo+1E6gbqtdTW5I6p8glWWLrjzRGMm1OpF8AyvPq/p39zYvvuY6cr/86s0aHd+6fdcVQXZpDCYHp7K+31hqdna2OBKSbX/d9+efatc/fbrtBZKtHuzxz/Q2/7aErrzwipg+xtH9wex1uXpWMsSCqE3oxGAQFLP5O6TD7/zd1y+fjslYpDgwvr+NMbFvrSz3vJxJqjza+bMn9ue/P83PXhpMovLueYMlPBnrOEIiBRCIZYSBJutbn/4GR2I7LGgL3dvbt3tW2Oc/JuUCx31/Yv7+8frb88n/G7+8Y2+WTDKK//i7U6hZbYGSI9uHTJuopRQ8h1S//VeZruVeNm5ozkgLL9hw1bsGb554scQFg+/OPMaPHsz+oiqWNVihxqsji7dcyDQVYc/a6T6YPVF1ejSWCAyDvXgI7W3lgIN4fj320ut7yydefyqQd3lGPxTXbw0kMQV0AQbySMlMpxToByPrF9OXn12QXymGyN67W5rh0CRQtzp3u9iPbow2vNvd355eeWc8L4VmB0HX7tOPanTwgUAGpuXZ8r8cpieBccHAXi7L/Wg90TG13nMZeQqvH/dWcdd+iJt56EcMYyOdjudO7tGamhSDBDxW0IDvojqwbJxt1nw1bn5t++JKIcjM7Tjf7Zz2d+fLZOMRlujQ/H/ZN6lx+7qIIRZWmFrbNkklRQEaGwJnR0q4GIZJoJJze9PLN10G5vxFtleW7xmb0vZu7b2GLqze77nzwoXh28tIbQ0XeEX23vUh6W0z8eREcRtr/ZxPiFy4mgsyQtBkQcbOLZonMbNFiHBu78+61L47D1nz2WPNoq0cHm5PTR+UUIhQDySGoOUo63GfJiwZ0PPhr0+MpLlyqC1CSN7KMhf82vPt0bjXTn2pczebHZnV6082M4v/oZLr6y0AtwJjRESAIdkQQaJmBIoPLEK+9Me/3Dz76421tLCz28WGGS7cP9jv3aSs03r984tDmYnHmuTh7fKSUgphtlfADnlqWEEHcb48Hu6i9mIpDsaHy41DvaP97av7h07br2Y7U3suG795emw/ri2yf6jtC39vDW7GnSK6SIhizQdckMStk71Kjx4+7DO23v5GuvnRDiiISZrQz99vGLr59sx5Pbtw7x4Oz84APZubC98vjgtY3q6TdYIwkEC0M2CHQGwKAuhNRdln956rM9HFf3Jgkvrs0vrIACLF5+EF378a6qVaUa+cIrT864skZ8q/vqZHeoZfHHA0QngzKE8bQjFpoc3b3+af/Hc59uVE88dpmrdCild3R96/5h6o/mH3vjMYpExXW0sXNuOnPm3KAlJ7VCgRpEYSUhd19eS7udwNO/nAXogphJdTK+uzleWZXhwdc793l3af0K3Qd5/GR6vM/PPVb1+0HOQRxRsiGrCqAqkwljgyiz1fyTw+OyunN89sRSCAsm9YWL7/SbYgcHXV6bHy++8PTpgRgkx+9wPf7gnna9lx+TQGgJ26No0K6OZmT346/2ptWDAXdPvbRYYVU1hPv/+cFk7qmD4crZF85x5HCiZuN6r1+NX+OYJXcCYRVQA+7P5lnWvUE2WLz8/ExmYydDRJTpF9fbxbJ//92uuY2ycmJ8fbK+srY+wwqDASGRGKAWFzQgLeQNJRCvARUCZQngAvz/zu7g/MsfPEjdbss6rdffeG4WGDl5ju/MtA6u+bitHs/eKU5nqeh0wuM/2GpnNjHqcWlXXl2pMJYf+9Rpey/kRH9y/tTTq1VuAmN6PPzTwTnDF04RYhiyKvSoJTRwIuqcc10/9vxzc4AAUZuFqneHd3bg4Ob4oKlub82ciEO08yuL6/3USnJwTt6yWs8SILmDgAZBAgzsXPCf32+SWPrV3HuTZpxy/fhzl5eqnF2R0fQ7O5UH+2WMp/MuHOHFFeSFpz9jbA4meYRCPZqdu3Lm1EoigGp9ZkITjurq4qUry0lSoCau6OZoc9o7c6WGQgKpI7JwRE8drSzvLuqUL71+ro8cmqJ4dG1pRsP3NqZ5OuoPJ4e7dGZxujg498p8QkJOYAbJo2qYjTBAOmMskAkUEhVA+q98x4Eu5waXh8N9p3MXZvuiaAwqRlK+5bo98UZvjh47jy/NicnshTf+YTgdpj11pLlzL59eJSbiQB7MGlUN4vD6z1dmAluTyg3x3HXei+dnMQEisXgYAAUa8OCFzepwxmF3rWPngk1jm4OxfgL1JwecdbeBYKfNnWfSk9WgMmeXyntWNEQJQDlFlKlkTmSuYoqGYPTP717OA8DK5X/ul++cut4H6w5w+c0VVEQ+9TcX/rDtnU76g5mfPz5bVSVSiMXgyX/5/mGp1xdWHl+rDAFTuEXG5V9cufbykgC5CZhydrcAwgK9Zz/oRm539/cuOU2obDWnPl9r07adL5StZ8e9PvR6F8aXn139ZgWiStQB1gqMKC0UZojMMXVmRccK2iDXH7CjN8+NMs49/cqJ5MTQ6cyV9WHjk8nC0sIsI6OYkSPE8k+f2hnV63P9lARKzxCdgZBOLl7qZ8ZCEBSIGG1mUYSMp/+Hd4ftZGb0dVm+t4/g0dkx6xleH1zbm0CDKLCw9tOzp2YTpw5zAISDYrARFRIAdeBoJJDDggsggKUf8D3d2Rcm9fwTb5zpoUryFpPW9TK06FIJgH/3A9gn/1mAbck5KdZYghURkQOFre9YPzHoXb213n9wc3Pf8uzplVtzJ9YvYhLfufmPx+N+tbbUf+HUUhVoxOKe3FydUaBABLYEjkGMSdwh1ImiAaC/4H9pHJhEACWTomB1kAagBTviJLrDr5d617afutNfxxO9iecBBQWUcvfOB/ODC6dPDHKEo6Rs05QUIzk6YxgEOiAbcScU5EhKoUZ98/8XWb8Dr8mdTNgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=235x76>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(image_path)\n",
    "display(img)\n",
    "\n",
    "print(len(processed_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co launch\n"
     ]
    }
   ],
   "source": [
    "hypothesis = OCR(image_path)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42803\n"
     ]
    }
   ],
   "source": [
    "index = find_key_index(filepaths_dic, sample)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'MOVE', 'to', 'stop', 'Mr.', 'Gaitskell', 'from', 'nominating', 'any', 'more', 'Labour', 'life', 'Peers', 'is', 'to', 'be', 'made', 'at', 'a', 'meeting', 'of', 'Labour', 'Ps', 'tomorrow', '.', 'Mr.', 'Michael', 'Foot', 'has', 'put', 'down', 'a', 'resolution', 'on', 'the', 'subject', 'and', 'he', 'is', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(words_lst[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: nominating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTesseract Hypothesis: twig\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m transformer_hypothesis \u001b[39m=\u001b[39m OCR(image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPyTesseract Hypothesis:\u001b[39m\u001b[39m\"\u001b[39m, py_hypothesis)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCER: \u001b[39m\u001b[39m\"\u001b[39m, cer(ground_truth, py_hypothesis)) \u001b[39m# char error rate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWER: \u001b[39m\u001b[39m\"\u001b[39m, wer(ground_truth, py_hypothesis)) \u001b[39m# word error rate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEdit distance: \u001b[39m\u001b[39m\"\u001b[39m, edit_distance(ground_truth, py_hypothesis)) \u001b[39m# edit distance\u001b[39;00m\n",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 25\u001b[0m in \u001b[0;36mcer\u001b[0;34m(ground_truth, hypothesis)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcer\u001b[39m(ground_truth, hypothesis):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# get character error rate\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ref_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(reference)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     hyp_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(hypothesis)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     distance \u001b[39m=\u001b[39m [[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m (hyp_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ref_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reference' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EXAMPLE using Pytesseract (less accurate)\n",
    "\n",
    "import pytesseract\n",
    "from fuzzywuzzy import fuzz\n",
    "from jiwer import wer\n",
    "\n",
    "i = 7\n",
    "\n",
    "ground_truth = words_lst[i] #'materials'\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "plt.imshow(processed_images[i], cmap=\"gray\")\n",
    "\n",
    "py_hypothesis = pytesseract.image_to_string(processed_images[i])\n",
    "\n",
    "sample = list(filepaths_dic.keys())[i]\n",
    "image_path = os.path.join('./Datasets/words', sample)\n",
    "transformer_hypothesis = OCR(image_path)\n",
    "\n",
    "print(\"PyTesseract Hypothesis:\", py_hypothesis)\n",
    "print(\"CER: \", cer(ground_truth, py_hypothesis)) # char error rate\n",
    "print(\"WER: \", wer(ground_truth, py_hypothesis)) # word error rate\n",
    "print(\"Edit distance: \", edit_distance(ground_truth, py_hypothesis)) # edit distance\n",
    "print(\"Levenshtein distance: \", levenshtein_distance(ground_truth, py_hypothesis)) # levenshtein distance\n",
    "print(\"________________\")\n",
    "\n",
    "print(\"TrOCR Transformer Hypothesis:\", transformer_hypothesis)\n",
    "print(\"CER: \", cer(ground_truth, transformer_hypothesis)) # char error rate\n",
    "print(\"WER: \", wer(ground_truth, transformer_hypothesis)) # word error rate\n",
    "print(\"Edit distance: \", edit_distance(ground_truth, transformer_hypothesis)) # edit distance\n",
    "print(\"Levenshtein distance: \", levenshtein_distance(ground_truth, transformer_hypothesis)) # levenshtein distance\n",
    "\n",
    "print(\"Ground Truth:\", ground_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating OCR Model on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./Datasets/words/e04/e04-127/e04-127-04-05.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-06-00.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-01-09.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-06-01.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-01-08.png\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================== Iterate through all images in subfolders recursively =======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def OCR_all(folder_path, model=pipe, MAX_ITER=float('inf'), progress=False, save_output=False):\n",
    "  '''\n",
    "  Function performs OCR processing on this file structure:\n",
    "  root\n",
    "    subfolders\n",
    "      sub-subfolders\n",
    "        images\n",
    "\n",
    "  Save output flag toggles creation of additional subfolders for preprocessed images (NOT APPLICABLE)\n",
    "  '''\n",
    "  \n",
    "  iter = 0\n",
    "\n",
    "  def process_subfolder(subfolder_path):\n",
    "    nonlocal iter\n",
    "    # Get a list of files in the folder\n",
    "    file_list = os.listdir(subfolder_path)\n",
    "\n",
    "    # Initialize tqdm with the total number of files\n",
    "    if progress:\n",
    "      progress_bar = tqdm(file_list, desc='Processing Images', unit='image')\n",
    "      iterator = progress_bar\n",
    "    else:\n",
    "      iterator = file_list\n",
    "      \n",
    "    for file_name in iterator:\n",
    "      if iter >= MAX_ITER:\n",
    "        break\n",
    "\n",
    "      # Check if the file has an image extension\n",
    "      if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "          img = os.path.splitext(file_name)[0]  # Get the image name without extension\n",
    "          image_path = os.path.join(subfolder_path, file_name)\n",
    "          \n",
    "          if save_output:\n",
    "            # processed output folder\n",
    "            output_folder = os.path.join(subfolder_path, img)\n",
    "\n",
    "            if not os.path.exists(output_folder): # folder for patient i with extracted data\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "          ########################### # do processing # ###########################\n",
    "\n",
    "          predict = OCR(image_path, model)\n",
    "          #print(\"Iteration: \", iter, \" | File:\", file_name)\n",
    "          print(\"Path:\", image_path)\n",
    "\n",
    "          iter += 1\n",
    "\n",
    "  def traverse_folder(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for dir in dirs:\n",
    "            subfolder_path = os.path.join(root, dir)\n",
    "            process_subfolder(subfolder_path)\n",
    "\n",
    "  traverse_folder(folder_path)\n",
    "\n",
    "  print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_path = './Datasets/words/'\n",
    "sentence_path = './Datasets/sentences'\n",
    "\n",
    "OCR_all(word_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cer, wer, edit_distance scores: [0.21856476856476856, 1.2727272727272727, 3.727272727272727]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>Edit Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CER  WER  Edit Distance\n",
       "0   0.000000  1.0              1\n",
       "1   0.000000  0.0              1\n",
       "2   0.000000  0.0              1\n",
       "3   0.500000  1.0              2\n",
       "4   0.000000  1.0              3\n",
       "5   0.583333  4.0              9\n",
       "6   0.200000  1.0              2\n",
       "7   0.692308  3.0             11\n",
       "8   0.000000  1.0              3\n",
       "9   0.000000  1.0              4\n",
       "10  0.428571  1.0              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# iterate through word dataset\n",
    "\n",
    "TEST_SIZE = 10\n",
    "\n",
    "word_scores = []\n",
    "word_score_model = []\n",
    "\n",
    "for i in range(len(processed_images)): \n",
    "    if i > TEST_SIZE:\n",
    "        break\n",
    "    ground_truth = words_lst[i]\n",
    "    hypothesis = pytesseract.image_to_string(processed_images[i])\n",
    "    cer_score, wer_score, edit_distance_score = get_accuracy_metrics(ground_truth, hypothesis)\n",
    "    word_scores.append([cer_score, wer_score, edit_distance_score])\n",
    "\n",
    "#### TODO #####\n",
    "\n",
    "sent_scores = [] \n",
    "sent_score_model = []\n",
    "\n",
    "for i in range(len(processed_images)): \n",
    "    if i > TEST_SIZE:\n",
    "        break\n",
    "    ground_truth = words_lst[i]\n",
    "    hypothesis = pytesseract.image_to_string(processed_images[i])\n",
    "    cer_score, wer_score, edit_distance_score = get_accuracy_metrics(ground_truth, hypothesis)\n",
    "    sent_scores.append([cer_score, wer_score, edit_distance_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_col_average(scores):\n",
    "    transposed_data = zip(*scores)\n",
    "    column_averages = [sum(column) / len(column) for column in transposed_data]\n",
    "    return column_averages\n",
    "\n",
    "\n",
    "print(\"average cer, wer, edit_distance scores:\", get_col_average(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>Edit Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CER  WER  Edit Distance\n",
       "0   0.000000  1.0              1\n",
       "1   0.000000  0.0              1\n",
       "2   0.000000  0.0              1\n",
       "3   0.500000  1.0              2\n",
       "4   0.000000  1.0              3\n",
       "5   0.583333  4.0              9\n",
       "6   0.200000  1.0              2\n",
       "7   0.692308  3.0             11\n",
       "8   0.000000  1.0              3\n",
       "9   0.000000  1.0              4\n",
       "10  0.428571  1.0              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(scores, columns=['CER', 'WER', 'Edit Distance'])\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning OCR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load and preprocess the image (you'll need to implement this)\n",
    "        image = preprocess_image(image_path)\n",
    "\n",
    "        return {\n",
    "            \"image\": image, # image path\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "# Example paths and labels\n",
    "image_paths = [\"path_to_image_1.jpg\", \"path_to_image_2.jpg\", ...]\n",
    "labels = [\"label_1\", \"label_2\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, pipeline, TrocrForImageToText\n",
    "\n",
    "# Load your labeled dataset (images and corresponding text labels)\n",
    "# You'll need to write code to read and preprocess this data\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = TrocrForImageToText.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Configure the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saved\",   # Directory to save the fine-tuned model\n",
    "    num_train_epochs=5,               # Number of training epochs\n",
    "    per_device_train_batch_size=4,    # Batch size\n",
    "    save_steps=1000,                  # Save model checkpoints every X steps\n",
    "    evaluation_strategy=\"steps\",      # Evaluate every X steps\n",
    "    eval_steps=500,                   # Evaluate every X steps\n",
    "    logging_steps=100,                # Log metrics every X steps\n",
    "    learning_rate=1e-4,               # Learning rate\n",
    "    do_train=True,                    # Start training\n",
    "    do_eval=True,                     # Evaluate during training\n",
    "    # Add more training arguments as needed\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=your_train_dataset,   # Your training dataset\n",
    "    eval_dataset=your_val_dataset,      # Your validation dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def reconstruction_loss(inputs, outputs):\n",
    "    reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "    reconstruction_loss *= (IMAGE_WIDTH * IMAGE_HEIGHT)  # Image size\n",
    "    return reconstruction_loss\n",
    "\n",
    "def kl_divergence_loss(z_mean, z_log_var):\n",
    "    # Kullback-Leiber Divergence Loss\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return kl_loss\n",
    "\n",
    "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
    "    reconstruction = reconstruction_loss(inputs, outputs)\n",
    "    kl_divergence = kl_divergence_loss(z_mean, z_log_var)\n",
    "    return reconstruction + kl_divergence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-type Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(image_labels))\n",
    "print(type([images]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "# Step 4: Model selection\n",
    "latent_dim = 100\n",
    "text_dim = 128\n",
    "image_dim = (IMAGE_WIDTH, IMAGE_HEIGHT, 1) # using grayscale\n",
    "\n",
    "# Generator model - makes an image from text and noise\n",
    "generator_input = layers.Input(shape=(latent_dim + text_dim,))\n",
    "# hidden layers\n",
    "x = layers.Dense(256)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(1024)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "generator_output = layers.Dense(image_dim, activation='tanh')(x)\n",
    "generator = tf.keras.Model(generator_input, generator_output)\n",
    "\n",
    "# Discriminator model - classification model to predict real vs. fake \n",
    "discriminator_input = layers.Input(shape=(image_dim,))\n",
    "x = layers.Dense(512)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "discriminator = tf.keras.Model(discriminator_input, discriminator_output)\n",
    "\n",
    "# Combined model\n",
    "gan_input = layers.Input(shape=(latent_dim + text_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "discriminator.trainable = False\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "def build_generator():\n",
    "    input_noise = layers.Input(shape=(latent_dim,))\n",
    "    input_text = layers.Input(shape=(text_dim,))\n",
    "\n",
    "    # Concatenate the noise and text input\n",
    "    combined_input = layers.Concatenate()([input_noise, input_text])\n",
    "\n",
    "    x = layers.Dense(256)(combined_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((8, 8, 4))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    output_image = layers.Conv2D(1, (3, 3), activation='tanh', padding='same')(x)\n",
    "\n",
    "    generator = models.Model(inputs=[input_noise, input_text], outputs=output_image, name='Generator')\n",
    "    return generator\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    input_image = layers.Input(shape=image_dim)\n",
    "    input_text = layers.Input(shape=(text_dim,))\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_image)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Concatenate the image features and text input\n",
    "    combined_input = layers.Concatenate()([x, input_text])\n",
    "\n",
    "    x = layers.Dense(256)(combined_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    output_pred = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    discriminator = models.Model(inputs=[input_image, input_text], outputs=output_pred, name='Discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'str\\'>\"})', \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m text_input \u001b[39m=\u001b[39m image_labels[batch:(batch \u001b[39m+\u001b[39m batch_size)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Generate images from noise and text input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# generator input shape should be (latent_dim + text_dim,)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m generated_images \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpredict([noise, text_input])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Combine real and generated images for the discriminator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m real_images \u001b[39m=\u001b[39m images[batch:(batch \u001b[39m+\u001b[39m batch_size)]  \u001b[39m# Custom function to get real images\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/keras/engine/data_adapter.py:1081\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1078\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1080\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1083\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1084\u001b[0m         )\n\u001b[1;32m   1085\u001b[0m     )\n\u001b[1;32m   1086\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1087\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1091\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'str\\'>\"})', \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Step 6: Model training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epochs', unit='epoch'):\n",
    "    for batch in tqdm(range(len(train) // batch_size), desc='Batches', unit='batch', leave=False): #range(len(train) // batch_size):\n",
    "        # Generate random noise and text input\n",
    "        noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "        text_input = image_labels[batch:(batch + batch_size)] # process string -> vector\n",
    "        # consider OneHot Encoding characters\n",
    "        \n",
    "        # Generate images from noise and text input\n",
    "        # generator input shape should be (latent_dim + text_dim,)\n",
    "        generated_images = generator.predict([noise, text_input])\n",
    "\n",
    "        # Combine real and generated images for the discriminator\n",
    "        real_images = images[batch:(batch + batch_size)]  # Custom function to get real images\n",
    "        combined_images = np.concatenate([real_images, generated_images])\n",
    "\n",
    "        # Labels for real and generated images\n",
    "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "        labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(combined_images)\n",
    "            discriminator_loss = loss(labels, predictions)\n",
    "        grads = tape.gradient(discriminator_loss, discriminator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "        # Train the generator (via the gan model)\n",
    "        noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "        text_input = image_labels[batch:(batch + batch_size)]  # Custom function to get text input\n",
    "        labels = np.ones((batch_size, 1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = generator([noise, text_input])\n",
    "            predictions = discriminator(generated_images)\n",
    "            generator_loss = loss(labels, predictions)\n",
    "        grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alt Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Model training\n",
    "def train_gan(gan, generator, discriminator, train_data, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(train_data) // batch_size):\n",
    "            noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "            text_input = train_data[batch * batch_size : (batch + 1) * batch_size, 1]\n",
    "\n",
    "            generated_images = generator.predict([noise, text_input])\n",
    "\n",
    "            real_images = train_data[batch * batch_size : (batch + 1) * batch_size, 0]\n",
    "            combined_images = np.concatenate([real_images, generated_images])\n",
    "\n",
    "            labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "            labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "            d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "            noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "            text_input = train_data[batch * batch_size : (batch + 1) * batch_size, 1]\n",
    "            labels = np.ones((batch_size, 1))\n",
    "\n",
    "            g_loss = gan.train_on_batch([noise, text_input], labels)\n",
    "\n",
    "            print(f\"Epoch {epoch}/{epochs}, Batch {batch}/{len(train_data) // batch_size}, D Loss: {d_loss}, G Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate sample images and evaluate the GAN model\n",
    "\n",
    "# Create a function to generate and save sample images\n",
    "def generate_images(model, epoch, noise_dim, num_examples_to_generate, text_input, dir='./generated'):\n",
    "    \"\"\"\n",
    "    Generate a 4x4 grid of images from a GAN model.\n",
    "    Args:\n",
    "        model: The GAN model that will be used to generate the images.\n",
    "        epoch: The epoch at which the images will be generated.\n",
    "        noise_dim: The dimensionality of the noise vector that will be used to generate the images.\n",
    "        num_examples_to_generate: The number of images that will be generated.\n",
    "        text_input: The text input that will be used to condition the GAN model.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Process the text input with the dimension of noise\n",
    "    predictions = model([noise_dim, text_input])\n",
    "    # Create a new figure with a size of 3 x 3 inches\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    # Loop over the generated images\n",
    "    for i in range(predictions.shape[0]):\n",
    "        # Create a subplot in the figure\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        # Plot the image in the subplot\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        # Set the axis of the subplot to `off`\n",
    "        plt.axis('off')\n",
    "        # Save the figure as a PNG file with the name `image_at_epoch_{epoch}.png`\n",
    "        #path = os.path(dir)\n",
    "        plt.savefig(f'./{dir}/image_at_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Define the noise dimension for generating sample images\n",
    "noise_dim = 100\n",
    "\n",
    "# Create random noise and text input for generating sample images\n",
    "num_examples_to_generate = 16\n",
    "noise = np.random.normal(size=(num_examples_to_generate, noise_dim))\n",
    "text_input = get_random_text_input(0, num_examples_to_generate)  # Custom function to get text input\n",
    "\n",
    "# Generate and save sample images using the generator model\n",
    "generate_and_save_images(generator, 0, noise_dim, num_examples_to_generate, text_input)\n",
    "\n",
    "# Evaluate the GAN model\n",
    "# You can define evaluation metrics or procedures here to assess the quality of generated images and discriminator performance.\n",
    "# For example, you can use an external evaluation metric like FID (Fréchet Inception Distance) or manually inspect the generated images.\n",
    "\n",
    "# Save the trained generator and discriminator models\n",
    "generator.save('gan_generator.h5')\n",
    "discriminator.save('gan_discriminator.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Generation - Generate new images based on input text\n",
    "input_text = \"Hello, World!\"\n",
    "noise = np.random.normal(size=(1, latent_dim))\n",
    "text_input = preprocess_text(input_text)  # Custom function to preprocess input text\n",
    "generated_image = generator.predict([noise, text_input])\n",
    "\n",
    "# Display or save the generated image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-type Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "path = \"/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/GAN_VAE_Model\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128  # Dimensionality of the latent space\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# Variational Autoencoder\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "# instantiate VAE model and loss metrics\n",
    "vae = VAE(encoder, decoder)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "vae.compile(optimizer, loss=vae_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(dataset, epochs=10, batch_size=64)\n",
    "\n",
    "random_latent_vectors = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "generated_images = decoder(random_latent_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_model = '/content/scrabble-gan/res/out/big_ac_gan/model/generator_' + str(epochs)\n",
    "\n",
    "# number of samples to generate\n",
    "n_samples = 10\n",
    "# your sample string\n",
    "sample_string = 'machinelearning'\n",
    "\n",
    "# load trained model\n",
    "imported_model = tf.saved_model.load(path_to_saved_model)\n",
    "\n",
    "# inference loop\n",
    "for idx in range(1):\n",
    "  fake_labels = []\n",
    "  words = [sample_string] * 10\n",
    "  noise = tf.random.normal([n_samples, latent_dim])\n",
    "  \n",
    "  # encode words\n",
    "  for word in words:\n",
    "    fake_labels.append([char_vec.index(char) for char in word])\n",
    "  fake_labels = np.array(fake_labels, np.int32)\n",
    "\n",
    "  # run inference process\n",
    "  predictions = imported_model([noise, fake_labels], training=False)\n",
    "  # transform values into range [0, 1]\n",
    "  predictions = (predictions + 1) / 2.0\n",
    "\n",
    "  # plot results\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(10, 1, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    # plt.text(0, -1, \"\".join([char_vec[label] for label in fake_labels[i]]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with GAN_VAE_Model Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run shell\n",
    "# python ./GAN_VAE_Model/generate.py -d ./generated\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Project States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete\n"
     ]
    }
   ],
   "source": [
    "# Save Stuff\n",
    "\n",
    "# Put processed images in new folder\n",
    "import pickle\n",
    "with open(\"./filtered_data/processed_images.pkl\", \"wb\") as file:\n",
    "    pickle.dump(processed_images, file)\n",
    "print(\"Save Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete\n"
     ]
    }
   ],
   "source": [
    "# Load Stuff\n",
    "with open(\"./filtered_data/processed_images.pkl\", \"rb\") as file:\n",
    "    (\n",
    "        processed_images\n",
    "    ) = pickle.load(file)\n",
    "print(\"Load Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115239"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
