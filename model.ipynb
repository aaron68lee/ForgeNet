{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Set\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "def getUniqueSetOfChars(words_lst: List[str]) -> List[str]:\n",
    "    '''\n",
    "    Find a list of unique chars in words_lst. This is needed for the one hot encoding to turn words into numerical vectors\n",
    "    '''\n",
    "    unique_chars = set()\n",
    "    for word in words_lst:\n",
    "        unique_chars.update(set(word))\n",
    "    return list(unique_chars)\n",
    "\n",
    "# print(getUniqueSetOfChars(words_lst))\n",
    "\n",
    "\n",
    "\n",
    "def oneHotEncodeStrings(unique_chars: List[str], strings: List[List[str]]) -> List[List[int]]:\n",
    "    char_indices = {char: i for i, char in enumerate(unique_chars)}\n",
    "    encoded_vectors = []\n",
    "    \n",
    "    for string_list in strings:\n",
    "        encoded_string = [\n",
    "            [1 if char in string else 0 for char in unique_chars]\n",
    "            for string in string_list\n",
    "        ]\n",
    "        encoded_vectors.append([val for sublist in encoded_string for val in sublist])\n",
    "    \n",
    "    return encoded_vectors\n",
    "\n",
    "\n",
    "\n",
    "test = ['apple', 'banana', 'orange']\n",
    "print(oneHotEncodeStrings(getUniqueSetOfChars(test) ,test))\n",
    "# print(len(oneHotEncodingCharacters(test)[0]))\n",
    "print(len(set(\"\".join(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew_process(image):\n",
    "\t# deskew image\n",
    "\timage = ioski.imread(sample_img)\n",
    "\tgrayscale = rgb2gray(image)\n",
    "\tangle = determine_skew(grayscale)\n",
    "\trotated = rotate(image, angle, resize=True) * 255\n",
    "\treturn rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# obsolete\\n# Step 5: Data preparation\\ndef get_random_text_input(start, batch_size): #Links \\n    \"\"\"\\n    Custom function to get text input for training\\n    \"\"\"\\n\\n    pass\\n\\ndef get_images(start, batch_size):\\n    \"\"\"\\n    Custom function to get real images for training\\n    \"\"\"\\n    pass\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'find_key_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m index \u001b[39m=\u001b[39m find_key_index(filepath_dic, sample)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#Y113sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_key_index' is not defined"
     ]
    }
   ],
   "source": [
    "def find_key_index(dictionary, key):\n",
    "    keys_list = list(dictionary.keys())\n",
    "    try:\n",
    "        index = keys_list.index(key)\n",
    "        return index\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # load variables.pkl\n",
    "import cv2 # image preprocessing\n",
    "import os\n",
    "\n",
    "IMAGE_DIR = os.getcwd() + \"/Datasets/words/\"\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_HEIGHT = 128\n",
    "\n",
    "# Step 1: Data collection - Prepare your labeled dataset\n",
    "with open(\"./variables.pkl\", \"rb\") as file:\n",
    "    (\n",
    "        filepaths_lst,\n",
    "        words_lst,\n",
    "        filepaths_dic,\n",
    "        words_dic,\n",
    "    ) = pickle.load(file)\n",
    "\n",
    "with open(\"./filtered_data/processed_images.pkl\", \"rb\") as file:\n",
    "    (\n",
    "        processed_images\n",
    "    ) = pickle.load(file)\n",
    "\n",
    "\n",
    "print(\"Load Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(image, TARGET_HEIGHT=100, TARGET_WIDTH=100):\n",
    "    \"\"\"This function will pre-process a image with: cv2 & deskew\n",
    "    so it can be process by tesseract\"\"\"\n",
    "    '''\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.resize(img, None, fx=.3, fy=.3) #resize using percentage\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #change color format from BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #format image to gray scale\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 5, 11) #to remove background\n",
    "    '''\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    # print(original_height, original_width)\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    if aspect_ratio >= 1.0:  # Landscape image\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:  # Portrait image\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image to the target size.\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Pad the resized image to make it the target size.\n",
    "    padded_image = np.ones((target_height, target_width), dtype=np.uint8) * 255\n",
    "    x_offset = (target_width - new_width) // 2\n",
    "    y_offset = (target_height - new_height) // 2\n",
    "    padded_image[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image_skew(image, TARGET_HEIGHT=100, TARGET_WIDTH=100):\n",
    "    '''\n",
    "    old preprocessing function that skews the original image\n",
    "    '''\n",
    "    #print(image_path)\n",
    "    height, width = image.shape\n",
    "    #print(f\"Image dimensions: {width} x {height}\")\n",
    "    image = cv2.resize(image, (target_width, target_height))  # Resize the image to a fixed size\n",
    "    image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image_tensor = image_tensor / 127.5 - 1.0  # Normalize pixel values between 0 and 1\n",
    "\n",
    "    # Add the preprocessed image and its corresponding label to the lists\n",
    "    processed_images.append(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_images = []\n",
    "bad_data = [\"a01/a01-117/a01-117-05-02.png\", \"r06/r06-022/r06-022-03-05.png\"]\n",
    "bad = []\n",
    "TARGET_HEIGHT, TARGET_WIDTH = 100, 100\n",
    "\n",
    "# TEST_SIZE = 10\n",
    "\n",
    "# Step 3: process the images into normlized 100*100 grayscale images\n",
    "for i, image_path in enumerate(filepaths_dic.keys()):\n",
    "    # if i > TEST_SIZE: \n",
    "    #     break\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    if image_path in bad_data: # bad data\n",
    "        continue\n",
    "    image = cv2.imread(IMAGE_DIR+image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        bad.append(image_path)\n",
    "        continue\n",
    "    height, width = image.shape\n",
    "    \"\"\"\n",
    "    change preprocess methods here\n",
    "    \"\"\"\n",
    "    padded_image = pre_process_image(image, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "    # Add the preprocessed image and its corresponding label to the lists\n",
    "    processed_images.append(padded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ec74fbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAptklEQVR4nO2deWxc133vP2dmOBu34b5KoixRmy3LkmU/eYHj1naa1IkdOG2joHkwHoIGaJvXtCjQpu/9Ubz/WqDoAuThoUbyAuM1XQwnsN3UiW2ojmVHNm3J1mZR1kJSEvcZksNl9uW8P8hzfDkaSpRJUVTu7wMIw3tn7tzfvfd8z+93fud3RkprjSAIv/p4brUBgiCsDSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkrErtS6gtKqU+UUheUUt9dLaMEQVh91GedZ1dKeYFzwBPAIPAB8HWt9ZnVM08QhNXCt4Jj7wcuaK37AJRS/wo8DSwp9sbGRt3V1bWCUwqCcC0GBgaIxWKq3HsrEXsHcMWxPQj8l9IPKaW+BXwLYOPGjRw9enQFpxQE4Vrs379/yfdWMmYv13tcNSbQWj+ntd6vtd7f1NS0gtMJgrASViL2QWCDY7sTGF6ZOYIg3CxWIvYPgG6l1GallB84CLyyOmYJgrDafOYxu9Y6r5T6NvAa4AX+r9b641WzTBCEVWUlCTq01q8Cr66SLYIg3ESkgk4QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcgohdEFyCiF0QXIKIXRBcwnXFrpTaoJR6UynVq5T6WCn1nYX99UqpN5RS5xde626+uYIgfFaW49nzwJ9qrXcCB4A/VErtAr4LHNJadwOHFrYFQVinXFfsWusRrfWHC3/PAr1AB/A08PzCx54HvnKTbBQEYRW4oTG7UqoL2Av0AC1a6xGY7xCA5iWO+ZZS6qhS6mg0Gl2huYIgfFaWLXalVBXwY+CPtdYzyz1Oa/2c1nq/1np/U1PTZ7FREIRVYFliV0pVMC/0H2mtf7Kwe0wp1bbwfhswfnNMFARhNVhONl4BPwB6tdZ/63jrFeDZhb+fBV5effMEQVgtfMv4zEPAfwVOKaWOL+z7H8BfAS8opb4JXAZ++6ZYKAjCqnBdsWut3wHUEm8/trrmCIJws5AKOkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCSJ2QXAJInZBcAkidkFwCcsWu1LKq5T6SCn104XteqXUG0qp8wuvdTfPTEEQVsqNePbvAL2O7e8Ch7TW3cChhW1BENYpyxK7UqoTeBL4vmP308DzC38/D3xlVS0TBGFVWa5n/3vgz4CiY1+L1noEYOG1udyBSqlvKaWOKqWORqPRldgqCMIKuK7YlVJfAsa11sc+ywm01s9prfdrrfc3NTV9lq8QBGEV8C3jMw8BTymlfhMIAjVKqX8CxpRSbVrrEaVUGzB+Mw0VBGFlXNeza63/QmvdqbXuAg4C/6m1/gbwCvDswseeBV6+aVYKgrBiVjLP/lfAE0qp88ATC9uCIKxTlhPGW7TWvwB+sfD3BPDYjZ5Qa73ke/l8nlwuRzqdJhaLkclkmJqaIpfLUSgUKBaLKKXsP4/n077K5/NRV1dHMBikvb2dqqqqGzVNEH6luSGxrxStNYVCYdG2k1QqxfT0NOPj4xw5coSJiQlOnjxJPB4nlUqRz+fx+Xx4vV77z1BdXc3dd99NS0sLX/7yl0XsglDCmordUCgUKBQKjI+PMz09TbFYpFgskkqlmJ2dZXp6msnJSWZmZgDw+/34/X4AlFLzhvt8+Hw+6/0BJiYm8Pl85HK5W3FZgrCuWXOxF4tFkskkqVSKl19+mXfffZd0Ok06nSabzZJKpQiFQnR2duL3+9m4cSOBQIDOzk5qa2tJpVKk02kqKirw+/1cvnyZw4cPk81muXDhAvF4nEQisdaXJQjrnjUVez6fJxaLEY/HSSaT1ntrrSkW5+t1vF4vfr+fqqoqwuEw9fX1BINB2traiEQiJJNJ0uk0Pp8Pv99PNpulpaWFZDJJIpEgm83a77qdKBaL5PN5isUi6XSaYrFIZWUlgUDgVpt223OtPJETEzWaz5vt5Xy3ySmlUinbBs2QtVgs4vF48Hg8+Hw+qqur8fl8eDyeZZ1jtVhTsUejUb73ve8xOjpKKpWitbWVAwcOsGnTJrq6uuzFe71eQqEQXq+XiooKu11RUWFvosfjwev1smvXLvbu3cvQ0BAvvPDCmgq9WCwuuyFdj1QqxZUrV5iZmeHEiRMkEgmeeOIJdu7cuegcq9E4jN3LaWylDV9rjda67PZybCsVlPP7l2tLOdvKJW211uTzeWtj6bFOu53nN23oWvfHHG866cnJSZLJJEeOHKG/v594PG6HqIVCgYqKCqqqqmhpaeGrX/0qra2thMNhKioqyp6j9D5da/9y28Saij2dTnPhwgVGRkZIp9O0trbS0tJCV1cXd955J16vF59vsUnOMbrzQs2DrampIRKJEAgEqKqqIp1OL3rgN5vVEns2m2ViYoKpqSn6+/uZnZ3lwIEDZRvqSnBGUdf7Xue5TdRhjq+oqLDPynzuRu67OcZ8//WOLWfrUkIo/YzJCcGnHtg5u+P1evF4PPj9fpRSixxGObvMeYvFIrlcjlwux/T0NNPT01y+fJnz589bsefzeZtYrqysJB6PE4/Hqa2txe/34/V6r9nRLfWMbnQ/rLHYc7kcU1NTdHd3U1NTw4EDB+ju7qa+vh6/32/DHCdOYTsfhLk5JuxvbW3l6aefJp/P09raupaXtcjW0sZ7vZDQHDMxMcHrr7/O6Ogog4ODaK2ZmZlZNN241PGG0s+U88pO28p5CWdDNrMn6XSaY8eOMTY2xszMDOl0mn379nHfffeV7aCvZeP1KI0cnPuc1+l8/1riNEI3wvzoo4+4dOkSiUSCRCKB3+8nFArR0tLCAw88QDgctiG2Ob7UFvOaTqcZHh4mHo9z6NAh68T8fj8PP/ww3d3dpNNpZmdnGR4e5u2332ZmZobBwUEAtmzZYj17aXRxI557ufd3TcVuxjQNDQ10dHSwYcMGOjs7qaiosGMaZ0N0PmTnzShtxH6/n+rqau6880601lRXV6/lZS3CNI5yYeO1BJtMJjl79ixDQ0MkEgkqKirs2P164SSUF/pS57zed5nQVGtNLpcjlUpx/vx5BgYGiMVizMzMUF9fz549e+wwa7XGnk4xOdvAUsMF53ulQi8NtXO5HJcuXeLkyZNMT08zMzNj286WLVu455578Pv9iyIW5z1ztkdzbyYnJxkfH+fUqVNcuXLF5pa2bNnCI488wtzcHJOTk5w5c4a3336bdDpNPB6nqqqKbDZLoVCwU8g38qw+C2s+z57JZDh37hxDQ0OMjIxQX19PKBQiFArR0NBAV1cXVVVVtLe3U1FRcdV3OL282QaoqKggEonYv9cS0wgKhQLZbNb+rZSyXsIIwtheOuY13sdMSyqlSKVS1vuYELP04S/ldSYmJhgfH6empob29nY8Hs+Snt5QKBTI5XIMDQ1x5MgREokEMzMzZDIZBgcHmZubswmo999/n4mJCUKhEJFIhMbGRu677z7C4XDZ0NeIpVxOZSl7nPfG2AdYceRyOYrF4iJP7CSfzzMxMcHc3Bw9PT2MjY3R19fH+Pg41dXVtLa22kSx1+vlrbfeorGxkf3791NbW1vWNnMdhUKByclJDh8+zPj4OEop2traePDBB9m8eTPt7e2k02n6+vro6elhdHSUfD5PJBKho6ODzs5OQqHQVde71D0p99yXymEsxZpPveVyOfr6+gDo7e3F6/VSVVVFVVUVd9xxBw8++CDNzc00Nzdb0TovdKkG7/P57AO6Xli52ji9h+mt0+k0SilCoRA+n8+Ga6YoqFySywg9n8/j8XjIZDIkk0kAO650FhKZazffAZ+G3xMTE1y4cIH29nZaW1uvSlyVNhalFIVCgUwmw5UrV/j3f/9322EUCgU7xjTfc/z4cY4ePUpNTQ2tra10d3eza9cu6xmddjmHNE6xl3Z6S425zXHOzLfW2t5rmH/m5TqveDzO+Pg4b7zxBufPn7fvdXd309TUxNjYGIODg2SzWXp6emhra2P79u1UVVXZ51Rqr+mY4/E47733HtFolI0bN9Lc3My+ffvYu3cvc3NzzM3NceXKFQ4fPkwmk6FYLBIIBGhubqatre2qmZZy0cu1wvlyHf21WFNVRCIRvvzlLzMwMMDMzAzj4+PMzs4C873wyMgIvb29zM3NsWPHDisSJ8tJypTS19fHxYsX7Y2MRCLs3LmTUCh0zeOXes/sN8mZCxcucPHiRbLZrA29TfgdCASsN/d4PGzfvp0NGzYQCAQIBAJW5H6/n66uLvx+PxcvXiSTydDf38+xY8fYunUrXV1dZe1xJtuKxSIXL15keHiY/v5+PvnkE3bu3ElXVxehUIhgMLhIeE4hFQoF+vv7OXnyJJcvXyaTyVBVVcVdd91FKBSyHru3t5fBwUFyuRyZTAalFH6/n5mZGSs8Z2M1nUixWKS/v5/Tp0/j9/uJRCJUV1ezdetWO/MCnxZcGZsGBgY4duwYtbW17N27l2AwuOg6SjsxJzMzM7z33nsMDw8zOTmJUopdu3bR2dlpHYrX62VgYMB2DKFQiHw+b20x53CKvFgsks1myWazZDIZstmsLfPu6elhYGDAJuaGh4epq6ujsrKSTZs20dTURENDw6KOs7RdXWuYVSrwGwnz11TsLS0t/MEf/AE///nPbXgzMjJCKpXC4/GQSCSIx+Ps2LGDhx9+mEAgYBNA5Xox452ud8HHjx/nxRdftJ/dvn07HR0dtuT2eqIuxRxjPO8777zDSy+9RCaTIZ1OA9iHaTorE5ofPHiQcDhsPaURWygU4q677qKhoYFoNEo0GuXkyZOMjY2hlGLTpk1lwzbj8bSen2Y6evQov/zlL+nv7+f8+fN87nOf4/7776eurs7aVOotTYM9ceIEP/zhD8nlcng8Hjo6Ovjd3/1dOjo68Hg85PN5vv/979vOIJFI2GObmpoWlUI7752JGI4ePcpzzz1HbW2t7fRaWloIBoOLsv6mI8lkMrz77rv89V//Nd3d3fz5n/85LS0t1NbW2qIq0zbKebbJyUleeuklrly5gsfjIRgM8sgjj/C5z33ORkoAx44dI51OMzY2hs/ns/PkpfkDZ6Ivk8mQSqVIJpO29sPr9fLTn/6UmZkZ2yk1NDSwYcMGtm7dyjPPPENVVZXNc1wr0rleGyy1bTmsqdjNNMf09DSxWAyPx0NdXR3Nzc20tLQQCoWoqalh48aNNit6o0Istz+TyRCPx+3NHR0d5cKFCzQ2NtLe3k4wGCzbYzrHZ87v93g8FItF+vr6GBwcZGhoiFwuRyQSoampaVEH5fV6KRQK9PX1MTs7y8jICGfPnmXTpk1UV1fbBpTJZKzIzTqAeDwOwNTUFIlEwnZ+peF/uXFdPp+3DfjYsWO0tbWVHVMXi0WbZBodHSWZTFJdXU13dzft7e1EIhF7f4xgQqGQDUuNl8vlcuTzedupOe00naKpfgyFQouGLsaTmiigUCgwOjrKlStXuHTpEtlslkQiwdDQEIVCgWAwaO+r1rqscJz4fD42bNhAXV0d9fX1i6LFuro6tm/fTiwW45NPPiGRSJBOp8lkMvYaTHswnjwejzMwMEBfXx/JZJJcLsfMzAy5XI6mpiY6OjqorKy0+YzW1labg3J2MqX5p3Id1lJO7rOw5mP2bDbLuXPn+PDDD2lqamLnzp08+uijPP7447YqzhQgLJXoudEOYG5ujmg0SjqdJplMMj09TTAYpKOjg6eeeuqqMa0Jwb1er81Gm8ZpogGtNa+++iqvvfaafWi7d+/m4MGDBAIBO4bM5/PMzs7ygx/8gBMnTvDBBx/wySef8Pjjj9PZ2YlSyhZlvP/++wwPDxOLxchms/T39+P1etm9ezfj4+PU1tYuGhY4OyKTpwgEAjbxk0wmOX78OIODg+zevZtNmzbR1tZ2VSd65swZ3n33XQYGBkgkEnR3d/PNb36Turo6ampqFgmrtraWpqYmMpkMsVjMhtupVMp6Y3MPYb6BxuNxpqam7Lyz1trmLszQxyTZMpkMuVyOnp4eXnnlFSYnJwGYnZ3lnXfeoa2tjfr6egKBgJ35MCIqzdV4PB7C4TCNjY188YtftIkz01kDbN68mYMHD3Ly5Ek++ugjW+swPT1NZWUlFRUVtiMx6zZOnjzJv/3bvxGPx4lGozYPFQgE+L3f+z0efPBBamtrqa6uvqrjN1GTU/TmPpnXcjNQ16uNuJYuDGsudjNlkc1mbaFBTU0NdXV1ixJZzga9Umpra9mwYQPRaJR4PM7c3BwjIyN2jDszM2NvlHO87fV6beLNLKE142yttZ1WMSGbz+ejpqbG/g3YZFsoFLLjdDPVaKYio9GoHc4UCgXq6uZ/lTsej1uP39fXR2NjI1prW05c6sm11oRCIerr6wmHwzZEn5ycJBaLEY1G8fv9dghhjp+bmyMWizE7O0s+n1+UNHXOIphIrKOjg9nZWYaGhuz5TQWgKV823hfmOx2zZsGMbWdmZojFYvT39xOLxawgjNgvX77M5OQkc3NzeDweCoWC/dzU1BTBYHBR9LdUWzHPyjgS80wNxrGEQiFbqnzlyhWUUrbCzbSJZDLJ7OwsExMTBINBqqqqaGpqIpvNEo/HKRaLzM3NMT09TXV1NeFw+KpOaKkx97Wy69dK0pV+z7VY86k3szYd5pelNjU1WS9ebmqq9CKud+HleOSRR9i6dSuvv/46f/d3f8fo6CgzMzMEAgGOHTuG3++3obOxzzwkMyXY3t7O7/zO79Da2kokEsHn8y3KwBthGfvMdZjGUl9fT1NTE7t27WLTpk1s3ryZfD7PmTNn+Od//mdbWVVVVcWv/dqvUVdXx3/8x39w+vRp3n77bU6ePMnmzZvZs2cPra2t7N+/n3A4bMNyk9S64447qKurY2JigrffftuG14ODg7z22mt0dnby0EMP0dDQYDvWoaEhTp06RTqdtqG4s+bBXE8gEOCBBx7gzjvv5KWXXrKJqEKhwNDQEN/73vdobm7mt37rt+js7KSmpoZAIEBfXx9nzpyxlWWzs7PEYjECgQBvvvkmPp/PdlaZTMYOQbLZrE0AptNpTp06xeDgIK2trXR0dHDnnXfS3Nxs20VpdrtYLJJIJKxAa2pqqKystB2RuUYTGQDEYjGee+45u6rSGQUYtmzZwle+8hVgfqXl9PQ0hw4dYnx8nDfffJOjR4/y5JNP8sQTTxAKhayHN8MH5zRoafbdWaoLVycJTbtfbkfg5JYscTU32Zl5dfJZMu7XIhQK0djYSFVVlZ2yMeFUMpnE4/FYsRvb/H4/gUDAhnJ1dXVX1cL7/X6CwSAw71lTqRRTU1Ok02nboILBILlczgrf/DNj8snJSTvEqK6upqqqio6ODhobG2lqaqKurs56Qq/XS319PYVCweY1jPc1ojNDDq/XSyAQsFFUKpViZGQEgLGxMbTWVFZW4vP5SKfTtlMwYbnJJjtDSeMhTXRiQlOYH54NDQ2RTqcZHR2loqKCTCZDMBgkFovZHyOprKxclBg0y5HNPtPZBgIB67nNsuXJyUk8Hg9jY2N4PB5aWlrw+XyEw2ECgYB9Fs52ZEJw43FTqZQ9p0k6miSjyT8MDw/b5+tcn+HsANra2uwwoaqqivr6euv54/E4Y2NjTExMEA6H7fWYIZGJHMvVPZj3boTljuXXPEFnBOL3++nv76e/v5/m5mbuu+8+Oz4qTfCUenrT8JbbKfT29nLkyBGOHz9uRWEeeCaTsSG71+vlwIED7N69m5aWFjo7O23NfTgcZsOGDQSDQetB9u7di1KKU6dOcerUKd5//31GRkZsKF1dXc3OnTupqKiwYj19+jQnTpwgEAjg9/sJh8Ps3buXSCTCrl27qK6upqWlxVZ2xWIxfvazn3Ho0CGGh4eZm5sjGAzy1ltvUVFRQSAQQCllC0ySySSZTAa/38/9999PNBrl7NmzTE9P84tf/IJgMMh7771HVVUV9957Lx0dHcTjcTvMMZVlFy9epKmpydY7mKjntdde4/jx41y6dIl8Pk8wGKSmpsZ2XqOjo/zoRz+yz9jr9drk3JYtW/j2t79NoVAgkUgsWtTU2tpqFzs5x7Qm7O/v7+eFF14gkUhw+PBh/H4/77zzDpWVldx7773s2rWLrVu30tjYaNuBiUaKxSI9PT2cPn2aZDLJ9u3bqaurIxKJMDQ0xMcff8zFixdtZ2AE3tXVRWNjI/v27WPbtm1W8LW1tbS3t6OUIp1O22PGx8d56623uHDhAm+99RZnzpyxSefNmzfzpS99yUYXJmdhIgcjchPyl6sGdHYE5cb411sEtuZiN2Mnv99vq5smJiZsttkUYTgXCDjFXm4sXywWy07RGSYnJ7lw4QLRaNR+ziScTCbYJNWam5vZtm0bGzduZNu2bVaQZohhbABoampiy5YtDA0N4fF4mJiYIBqNEgqFaG5uJhKJEA6HqaystFNyJlw3dHV1sX37dlpbW9mzZ8+ixKTJepu5aVMRprW240pzzc7lsZlMhp07d7Jnzx4ABgYGyGazxGIxAEZHR23Uks/nSSaTdpGG8bhTU1N4vV7rNU0m+tKlS3z88cc2qRYMBqmtrbXRRy6XY2BgwD5HpZSNBMLhMHv27CGfzzM9PW0rDj0eD3fccYfNdziLqZxRTTgcJpVKEYvF0FrbMbwpUjEFSM72ZqbnxsfHrSDN/HogEGBycpJLly4xMjJii3RMG6qurqahoYHt27fbdQDOWnYTHWUyGbZs2UIkEuHEiRN4PB47u2FyC5lMhoceesi2HzN16By2lpZGl866lFIulL8Wax7GBwIB7r//fpqamjh8+DC9vb289957TE1NWS/qDJ1Msst4CRNeAtYzmCmPL37xi9TX119VZWYScjU1NTz55JOEw2Ha29tJJpN2rDU3N0cul+PYsWMMDQ2xa9cu7rvvPtrb29mzZ489v/PhdHV12RV37e3tDA4O0tvba8WaTCb56KOP7HAhn8+zbds2OzVTU1Njy1nD4bAtFjHfb/Z9/vOfp6ury4bXuVyORCJBPp9nbm4OmO94wuGw9RImRzAzM8MXvvAFUqkUExMTzMzM0NPTQzwep6+vj7GxMZtFNz/6MTIywo9//GPrtU3nCPNTVU8++SSVlZV2zjgUClmbnEMd02Gb+9be3s7GjRttuGyeocfjobq62tYdwKcRnBlW7dixg9///d+303eFQoGZmRmy2Sx33303W7dupbm5eVFn39DQwDPPPMPo6Ci//OUviUajfPzxx/T19dmk6uTkJFeuXLFTghUVFTYHE41GF91jpwidFXvmx1UaGxs5ePAgjz32GGNjY8RiMYaHhzl37hwXL17k+eeft5Gtz+cjEokQCoXYtWsXGzduXJQUdXYqTkqLoZaThTesudh9Ph/btm2jrq6O3t5eent7OXfuHL29vbZxmflcs9DfeFdT8GDGVyYkT6fTbN68mYcffphIJHJVdjKVShGPx9m0aRP79++nubmZe+65h6mpKfr6+qyHyWQy9PX1ceHCBTvfXCwW2b1796KhhfnX0tJiE1319fWcOnWK0dHRRUME8/3Gu3V0dHDgwAEaGhpoaWlZFMXA4ukWM812zz33cPfddy/6Qc7JyUnS6TTj4+Nordm+fTv19fWLFhSZztHMU1++fJnR0VEuXbpEKpVibGyMXC5nG485Jh6P8+6779pGbaioqODrX/869957L21tbTaUhU87XvhU5OWSrU4Rm31OMpmM9a7OazE/YGKil1wux+DgILOzs2zcuNEW2ziprq7mwIEDdjbDlK/Ozs4uyhuYqDISieD3+23UZ1YdplKpRXYa25zRS0NDAwCtra0Ui0W7cOjDDz/k9OnTjI6OMjw8vOiHLFpbW6mursbj8VBZWWkrKZ21/qWLZJwhe2kkcD0Pf0vEbpa0PvbYY3R1dXH58mWb2TW9tulNTWWSEb+50ZWVlTQ3N1NZWUlLSwttbW2LsvpOdu3axVe/+lVbzWSy2EopfuM3foN9+/Zx9uxZYrHYopBuYGDALq4xD9f53SYca2hosD+wUV9fb8dOpmE655V37NhhbQiFQouy3qXhmzMMNmvITeLQ7/eTy+VsA29sbLTz686ZDXOMmWpraGjga1/7GlNTU7azLPWmzgIV5/30eDzs2bOHrq4uO7XkpDSTXE7QpddY+qzMnL453lkhaP42Sb5AIEAmkyESiVBVVXVVgs6M2SORCA899BDbtm1jdnbWtrFisciFCxfo6emxn9VaU1FRYZPHzl8+cnZgTrtMQtR5/0zexbTTcqW2586dY3p6mg8//JDBwUGbZHQmA020YSJI00GYnIhxQrlczuZrluKWiL2xsZH6+nqam5vJZrMcOXKEw4cPMzExYeuUzZjO3EDT45nQtqamhpaWFjZs2MCDDz5ITU0N1dXVZUOau+++2yZYTGPK5XKEw2Geeuopu4rLLFow5Y8XL160a+NLEyPwaa/b2NhIY2MjW7duteOypfgsMwulwxLAdkI3Qn19PQC7d+++4WMNK50ZWU1aWlqu+b5SyjqJRx991M7EOGcbDh06RG9vL1prgsEghULBRjOmnTg7H2cNvzOUdkYhMO/hW1tb2bFjB48++ijw6S/nzM7OMjU1xT/+4z8yNDRET08PiUTC2mW+z+/325qJTZs2EYlE2LdvH52dndTV1VFbW2vPmUwmiUajdnhUjjVP0Blv46x+MuH13Nwc27ZtI5vNMjc3ZzO1gA1tjFerr6+3YbRZJmsyueW8hclaOx+Ywefz0dnZace8Zo19IBBg27ZtV5WpXu8a1zu3g42rgfN5G49rvLdZxGSmzsw8P3za1pxtxVQrOocYsDiKckYi5ebEjeMyBTz79u2jsbHRrjEw058mWjC/gGNW2JkCo4GBASorK21kZWZjEomEjYjLseaevfT33rXW7N+/n7179y4KH69XJ1x6k6+VqDAhkRPz09TmXPfccw9aax555JFFUxjml3CE2w9nnsV4YlPnb+oPampqaG5uJpFIWGGZhTkmZ2GmAI0IzdAIFrdDg3NcbkJ3Z67C5/NRVVXF1772Nfu+s7LUzP2PjIzws5/9jLGxMc6cOcP09DQffPDBoo7JWSEYDAaZmJhY8n7ckqIaJ+aBlAtV19oGWPu18MLNx+kIjIiNWGpra9myZQvxeNyu5pubmyubn3E6Fud3O7/Xma9xbpezx0y9GbE7hxdmXH/HHXcQiUTweDw23+DseJwzH8FgkOnp6SXvg7Rs4VcaU6lpRGm8ovH03d3dtLW1cfnyZV599VVisRgXL14kmUzajt8kzEz1o9OLO8fzBiNAM0Vs9pVmzZ0/J20y8YFAwIreVFOaGQFnMrV0SGGGHM8888yS90LELvxK4wyfS4tQTC2DKXppb2+3dfimKs7v91NTU2N/Tde5Wq3cbEO5WvrS4WXpbItzuOFM8plZJ/j0V3HN/qX+78Nr/SSbiF34lcUkdKF8tZnz73A4THNzs53CcnrilpYWIpHIVTUDy+FaC1VKPb3z/dKfZFtqRVzpd12rrl7ELvxKs9xFJWap662gXMexVMewEtbuf1MQBOGWImIXBJcgYhcElyBiFwSXIGIXBJewLLErpSJKqReVUmeVUr1KqQeUUvVKqTeUUucXXututrGCIHx2luvZ/wH4udZ6B7AH6AW+CxzSWncDhxa2BUFYp1xX7EqpGuAR4AcAWuus1joOPA08v/Cx54Gv3BwTBUFYDZbj2e8AosAPlVIfKaW+r5SqBFq01iMAC6/N5Q5WSn1LKXVUKXU0Go2umuGCINwYyxG7D9gH/B+t9V4gwQ2E7Frr57TW+7XW+5uamj6jmYIgrJTliH0QGNRa9yxsv8i8+MeUUm0AC6/jN8dEQRBWg+uKXWs9ClxRSm1f2PUYcAZ4BXh2Yd+zwMs3xUJBEFaF5S6E+e/Aj5RSfqAP+G/MdxQvKKW+CVwGfvvmmCgIwmqwLLFrrY8D+8u89diqWiMIwk1DKugEwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSWI2AXBJYjYBcEliNgFwSUsS+xKqT9RSn2slDqtlPoXpVRQKVWvlHpDKXV+4bXuZhsrCMJn57piV0p1AH8E7Nda3wV4gYPAd4FDWutu4NDCtiAI65TlhvE+IKSU8gFhYBh4Gnh+4f3nga+sunWCIKwa1xW71noI+BvgMjACTGutXwdatNYjC58ZAZrLHa+U+pZS6qhS6mg0Gl09ywVBuCGWE8bXMe/FNwPtQKVS6hvLPYHW+jmt9X6t9f6mpqbPbqkgCCtiOWH840C/1jqqtc4BPwEeBMaUUm0AC6/jN89MQRBWynLEfhk4oJQKK6UU8BjQC7wCPLvwmWeBl2+OiYIgrAa+631Aa92jlHoR+BDIAx8BzwFVwAtKqW8y3yH89s00VBCElXFdsQNorf8S+MuS3RnmvbwgCLcBUkEnCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5BxC4ILkHELgguQcQuCC5Baa3X7mRKRYEEEFuzk66cRm4fe28nW+H2svd2sXWT1rqp3BtrKnYApdRRrfX+NT3pCrid7L2dbIXby97bydalkDBeEFyCiF0QXMKtEPtzt+CcK+F2svd2shVuL3tvJ1vLsuZjdkEQbg0SxguCSxCxC4JLWDOxK6W+oJT6RCl1QSn13bU673JRSm1QSr2plOpVSn2slPrOwv56pdQbSqnzC691t9pWg1LKq5T6SCn104Xt9WxrRCn1olLq7MI9fmC92quU+pOFNnBaKfUvSqngerX1RlgTsSulvMD/Br4I7AK+rpTatRbnvgHywJ9qrXcCB4A/XLDxu8AhrXU3cGhhe73wHaDXsb2ebf0H4Oda6x3AHubtXnf2KqU6gD8C9mut7wK8wEHWoa03jNb6pv8DHgBec2z/BfAXa3HuFdj8MvAE8AnQtrCvDfjkVtu2YEsn843u14GfLuxbr7bWAP0sJIQd+9edvUAHcAWoB3zAT4HPr0dbb/TfWoXx5gYaBhf2rUuUUl3AXqAHaNFajwAsvDbfQtOc/D3wZ0DRsW+92noHEAV+uDDs+L5SqpJ1aK/Wegj4G+AyMAJMa61fZx3aeqOsldhVmX3rcs5PKVUF/Bj4Y631zK22pxxKqS8B41rrY7falmXiA/YB/0drvZf59RHrMgxeGIs/DWwG2oFKpdQ3bq1Vq8NaiX0Q2ODY7gSG1+jcy0YpVcG80H+ktf7Jwu4xpVTbwvttwPitss/BQ8BTSqkB4F+BX1dK/RPr01aYf/6DWuuehe0XmRf/erT3caBfax3VWueAnwAPsj5tvSHWSuwfAN1Kqc1KKT/zCY9X1ujcy0IppYAfAL1a6791vPUK8OzC388yP5a/pWit/0Jr3am17mL+Xv6n1vobrENbAbTWo8AVpdT2hV2PAWdYn/ZeBg4opcILbeIx5pOJ69HWG2MNEx+/CZwDLgL/81YnK8rY9zDzQ4uTwPGFf78JNDCfCDu/8Fp/q20tsftRPk3QrVtbgXuAowv39yWgbr3aC/wv4CxwGvh/QGC92noj/6RcVhBcglTQCYJLELELgksQsQuCSxCxC4JLELELgksQsQuCSxCxC4JL+P8ZGOAlY7qUjwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test processed_image by plot out one processed image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(words_lst[50])\n",
    "plt.imshow(processed_images[50], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 115239\n",
      "Train set size: 92191\n",
      "Test set size: 11524\n",
      "Validation set size: 11524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Data preparation\n",
    "# Prepare your training dataset with input text and corresponding preprocessed images\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Variable \"processed_images_to_text\" holds pairs of images (tensor arrays) to their text label\n",
    "# Ex. [tensor array for \"Exchange\", 'Exchange']\n",
    "processed_images_to_text = []\n",
    "for i in range(len(processed_images)):\n",
    "    processed_images_to_text.append([processed_images[i], words_lst[i]])\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train, test = train_test_split(processed_images_to_text, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the test set further into test and validation sets (50% test, 50% validation)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(\"Dataset Size:\", len(processed_images))\n",
    "print(\"Train set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))\n",
    "print(\"Validation set size:\", len(val))\n",
    "\n",
    "image_labels = [train[i][1] for i in range(len(train))] #List of all the text labels after shuffling \n",
    "images = [train[i][0] for i in range(len(train))] #List of all the image tensors after shuffling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/opt/miniconda3/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-base-handwritten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import Levenshtein\n",
    "\n",
    "def cer(ground_truth, hypothesis):\n",
    "    # get character error rate\n",
    "    cer = sum(1 for a, b in zip(ground_truth, hypothesis) if a != b) / max(len(ground_truth), len(hypothesis))\n",
    "    return cer\n",
    "\n",
    "def edit_distance(ground_truth, hypothesis):\n",
    "    edit_dist = editdistance.eval(ground_truth, hypothesis)\n",
    "    return edit_dist\n",
    "\n",
    "def levenshtein_distance(ground_truth, hypothesis):\n",
    "    levenshtein_dist = Levenshtein.distance(ground_truth, hypothesis)\n",
    "    return levenshtein_dist\n",
    "\n",
    "def get_accuracy_metrics(ground_truth, hypothesis):\n",
    "    return cer(ground_truth, hypothesis), wer(ground_truth, hypothesis), edit_distance(ground_truth, hypothesis)\n",
    "\n",
    "def OCR(image_path, model=pipe): # can define custom OCR pretrained model to use #, model_path=\"./SimpleHTR-master/model\"):\n",
    "\n",
    "    '''\n",
    "    # Load the HTR-DeepWriting model\n",
    "    char_list_path = os.path.join(model_path, 'charList.txt')\n",
    "    model = Model(open(char_list_path).read(), decoderType=DecoderType.BestPath, mustRestore=True)\n",
    "\n",
    "    # Perform word segmentation\n",
    "    word_images = word_segmentation(image)\n",
    "\n",
    "    # Recognize handwriting for each word image\n",
    "    recognized_text = []\n",
    "    for word_img in word_images:\n",
    "        word_text = model.inferWord(word_img)\n",
    "        recognized_text.append(word_text)\n",
    "\n",
    "    # Join recognized words into a single string\n",
    "    final_text = ' '.join(recognized_text)\n",
    "\n",
    "    print(\"Recognized Text:\")\n",
    "    print(final_text)\n",
    "    '''\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    result = model(image_path)\n",
    "    text = result[0][\"generated_text\"]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: coloured\n",
      "./Datasets/words/e04/e04-127/e04-127-04-05.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = 'e04/e04-127/e04-127-04-05.png'\n",
    "image_path = os.path.join('./Datasets/words', sample)\n",
    "\n",
    "ground_truth = filepaths_dic[sample] # words_lst[i]\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=235x76>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co launch\n"
     ]
    }
   ],
   "source": [
    "hypothesis = OCR(image_path)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m index \u001b[39m=\u001b[39m find_key_index(filepath_dic, sample)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#Y113sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filepath_dic' is not defined"
     ]
    }
   ],
   "source": [
    "index = find_key_index(filepaths_dic, sample)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfuzzywuzzy\u001b[39;00m \u001b[39mimport\u001b[39;00m fuzz\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjiwer\u001b[39;00m \u001b[39mimport\u001b[39;00m wer\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m i \u001b[39m=\u001b[39m index\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m words_lst[i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGround Truth:\u001b[39m\u001b[39m\"\u001b[39m, words_lst[i])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# EXAMPLE using Pytesseract (less accurate)\n",
    "\n",
    "import pytesseract\n",
    "from fuzzywuzzy import fuzz\n",
    "from jiwer import wer\n",
    "\n",
    "i = index\n",
    "\n",
    "ground_truth = words_lst[i]\n",
    "print(\"Ground Truth:\", words_lst[i])\n",
    "plt.imshow(processed_images[i], cmap=\"gray\")\n",
    "\n",
    "hypothesis = pytesseract.image_to_string(processed_images[i])\n",
    "print(\"Hypothesis:\", hypothesis)\n",
    "print(\"CER: \", cer(ground_truth, hypothesis)) # char error rate\n",
    "print(\"WER: \", wer(ground_truth, hypothesis)) # word error rate\n",
    "print(\"Edit distance: \", edit_distance(ground_truth, hypothesis)) # edit distance\n",
    "print(\"Levenshtein distance: \", levenshtein_distance(ground_truth, hypothesis)) # levenshtein distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating OCR Model on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./Datasets/words/e04/e04-127/e04-127-04-05.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-06-00.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-01-09.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-06-01.png\n",
      "Path: ./Datasets/words/e04/e04-127/e04-127-01-08.png\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================== Iterate through all images in subfolders recursively =======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def OCR_all(folder_path, MAX_ITER=float('inf'), progress=False, save_output=False):\n",
    "  '''\n",
    "  Function performs processing on this file structure:\n",
    "  root\n",
    "    subfolders\n",
    "      sub-subfolders\n",
    "        images\n",
    "\n",
    "  Save output flag toggles creation of additional subfolders for preprocessed images (NOT APPLICABLE)\n",
    "  '''\n",
    "  \n",
    "  iter = 0\n",
    "\n",
    "  def process_subfolder(subfolder_path):\n",
    "    nonlocal iter\n",
    "    # Get a list of files in the folder\n",
    "    file_list = os.listdir(subfolder_path)\n",
    "\n",
    "    # Initialize tqdm with the total number of files\n",
    "    if progress:\n",
    "      progress_bar = tqdm(file_list, desc='Processing Images', unit='image')\n",
    "      iterator = progress_bar\n",
    "    else:\n",
    "      iterator = file_list\n",
    "      \n",
    "    for file_name in iterator:\n",
    "      if iter >= MAX_ITER:\n",
    "        break\n",
    "\n",
    "      # Check if the file has an image extension\n",
    "      if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "          img = os.path.splitext(file_name)[0]  # Get the image name without extension\n",
    "          image_path = os.path.join(subfolder_path, file_name)\n",
    "          \n",
    "          if save_output:\n",
    "            # processed output folder\n",
    "            output_folder = os.path.join(subfolder_path, img)\n",
    "\n",
    "            if not os.path.exists(output_folder): # folder for patient i with extracted data\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "          # do processing\n",
    "          #print(\"Iteration: \", iter, \" | File:\", file_name)\n",
    "          print(\"Path:\", image_path)\n",
    "\n",
    "          iter += 1\n",
    "\n",
    "  def traverse_folder(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for dir in dirs:\n",
    "            subfolder_path = os.path.join(root, dir)\n",
    "            process_subfolder(subfolder_path)\n",
    "\n",
    "  traverse_folder(folder_path)\n",
    "\n",
    "  print(\"Processing completed.\")\n",
    "\n",
    "word_path = './Datasets/words/'\n",
    "OCR_all(word_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cer, wer, edit_distance scores: [0.21856476856476856, 1.2727272727272727, 3.727272727272727]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>Edit Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CER  WER  Edit Distance\n",
       "0   0.000000  1.0              1\n",
       "1   0.000000  0.0              1\n",
       "2   0.000000  0.0              1\n",
       "3   0.500000  1.0              2\n",
       "4   0.000000  1.0              3\n",
       "5   0.583333  4.0              9\n",
       "6   0.200000  1.0              2\n",
       "7   0.692308  3.0             11\n",
       "8   0.000000  1.0              3\n",
       "9   0.000000  1.0              4\n",
       "10  0.428571  1.0              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# iterate through dataset\n",
    "\n",
    "scores = []\n",
    "for i in range(len(processed_images)): \n",
    "    if i > TEST_SIZE:\n",
    "        break\n",
    "    ground_truth = words_lst[i]\n",
    "    hypothesis = pytesseract.image_to_string(processed_images[i])\n",
    "    cer_score, wer_score, edit_distance_score = get_accuracy_metrics(ground_truth, hypothesis)\n",
    "    scores.append([cer_score, wer_score, edit_distance_score]) \n",
    "\n",
    "def get_col_average(scores):\n",
    "    transposed_data = zip(*scores)\n",
    "    column_averages = [sum(column) / len(column) for column in transposed_data]\n",
    "    return column_averages\n",
    "\n",
    "\n",
    "print(\"average cer, wer, edit_distance scores:\", get_col_average(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CER</th>\n",
       "      <th>WER</th>\n",
       "      <th>Edit Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CER  WER  Edit Distance\n",
       "0   0.000000  1.0              1\n",
       "1   0.000000  0.0              1\n",
       "2   0.000000  0.0              1\n",
       "3   0.500000  1.0              2\n",
       "4   0.000000  1.0              3\n",
       "5   0.583333  4.0              9\n",
       "6   0.200000  1.0              2\n",
       "7   0.692308  3.0             11\n",
       "8   0.000000  1.0              3\n",
       "9   0.000000  1.0              4\n",
       "10  0.428571  1.0              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(scores, columns=['CER', 'WER', 'Edit Distance'])\n",
    "display(score_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def reconstruction_loss(inputs, outputs):\n",
    "    reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "    reconstruction_loss *= (IMAGE_WIDTH * IMAGE_HEIGHT)  # Image size\n",
    "    return reconstruction_loss\n",
    "\n",
    "def kl_divergence_loss(z_mean, z_log_var):\n",
    "    # Kullback-Leiber Divergence Loss\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return kl_loss\n",
    "\n",
    "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
    "    reconstruction = reconstruction_loss(inputs, outputs)\n",
    "    kl_divergence = kl_divergence_loss(z_mean, z_log_var)\n",
    "    return reconstruction + kl_divergence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-type Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(image_labels))\n",
    "print(type([images]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "# Step 4: Model selection\n",
    "latent_dim = 100\n",
    "text_dim = 128\n",
    "image_dim = (IMAGE_WIDTH, IMAGE_HEIGHT, 1) # using grayscale\n",
    "\n",
    "# Generator model - makes an image from text and noise\n",
    "generator_input = layers.Input(shape=(latent_dim + text_dim,))\n",
    "# hidden layers\n",
    "x = layers.Dense(256)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(1024)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "generator_output = layers.Dense(image_dim, activation='tanh')(x)\n",
    "generator = tf.keras.Model(generator_input, generator_output)\n",
    "\n",
    "# Discriminator model - classification model to predict real vs. fake \n",
    "discriminator_input = layers.Input(shape=(image_dim,))\n",
    "x = layers.Dense(512)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "discriminator = tf.keras.Model(discriminator_input, discriminator_output)\n",
    "\n",
    "# Combined model\n",
    "gan_input = layers.Input(shape=(latent_dim + text_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "discriminator.trainable = False\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "def build_generator():\n",
    "    input_noise = layers.Input(shape=(latent_dim,))\n",
    "    input_text = layers.Input(shape=(text_dim,))\n",
    "\n",
    "    # Concatenate the noise and text input\n",
    "    combined_input = layers.Concatenate()([input_noise, input_text])\n",
    "\n",
    "    x = layers.Dense(256)(combined_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((8, 8, 4))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    output_image = layers.Conv2D(1, (3, 3), activation='tanh', padding='same')(x)\n",
    "\n",
    "    generator = models.Model(inputs=[input_noise, input_text], outputs=output_image, name='Generator')\n",
    "    return generator\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    input_image = layers.Input(shape=image_dim)\n",
    "    input_text = layers.Input(shape=(text_dim,))\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_image)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Concatenate the image features and text input\n",
    "    combined_input = layers.Concatenate()([x, input_text])\n",
    "\n",
    "    x = layers.Dense(256)(combined_input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    output_pred = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    discriminator = models.Model(inputs=[input_image, input_text], outputs=output_pred, name='Discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'str\\'>\"})', \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m text_input \u001b[39m=\u001b[39m image_labels[batch:(batch \u001b[39m+\u001b[39m batch_size)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Generate images from noise and text input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# generator input shape should be (latent_dim + text_dim,)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m generated_images \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpredict([noise, text_input])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Combine real and generated images for the discriminator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/model.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m real_images \u001b[39m=\u001b[39m images[batch:(batch \u001b[39m+\u001b[39m batch_size)]  \u001b[39m# Custom function to get real images\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/keras/engine/data_adapter.py:1081\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1078\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1080\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1083\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1084\u001b[0m         )\n\u001b[1;32m   1085\u001b[0m     )\n\u001b[1;32m   1086\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1087\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1091\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'str\\'>\"})', \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Step 6: Model training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc='Epochs', unit='epoch'):\n",
    "    for batch in tqdm(range(len(train) // batch_size), desc='Batches', unit='batch', leave=False): #range(len(train) // batch_size):\n",
    "        # Generate random noise and text input\n",
    "        noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "        text_input = image_labels[batch:(batch + batch_size)] # process string -> vector\n",
    "        # consider OneHot Encoding characters\n",
    "        \n",
    "        # Generate images from noise and text input\n",
    "        # generator input shape should be (latent_dim + text_dim,)\n",
    "        generated_images = generator.predict([noise, text_input])\n",
    "\n",
    "        # Combine real and generated images for the discriminator\n",
    "        real_images = images[batch:(batch + batch_size)]  # Custom function to get real images\n",
    "        combined_images = np.concatenate([real_images, generated_images])\n",
    "\n",
    "        # Labels for real and generated images\n",
    "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "        labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(combined_images)\n",
    "            discriminator_loss = loss(labels, predictions)\n",
    "        grads = tape.gradient(discriminator_loss, discriminator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "        # Train the generator (via the gan model)\n",
    "        noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "        text_input = image_labels[batch:(batch + batch_size)]  # Custom function to get text input\n",
    "        labels = np.ones((batch_size, 1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = generator([noise, text_input])\n",
    "            predictions = discriminator(generated_images)\n",
    "            generator_loss = loss(labels, predictions)\n",
    "        grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alt Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Model training\n",
    "def train_gan(gan, generator, discriminator, train_data, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(train_data) // batch_size):\n",
    "            noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "            text_input = train_data[batch * batch_size : (batch + 1) * batch_size, 1]\n",
    "\n",
    "            generated_images = generator.predict([noise, text_input])\n",
    "\n",
    "            real_images = train_data[batch * batch_size : (batch + 1) * batch_size, 0]\n",
    "            combined_images = np.concatenate([real_images, generated_images])\n",
    "\n",
    "            labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "            labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "            d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "            noise = np.random.normal(size=(batch_size, latent_dim))\n",
    "            text_input = train_data[batch * batch_size : (batch + 1) * batch_size, 1]\n",
    "            labels = np.ones((batch_size, 1))\n",
    "\n",
    "            g_loss = gan.train_on_batch([noise, text_input], labels)\n",
    "\n",
    "            print(f\"Epoch {epoch}/{epochs}, Batch {batch}/{len(train_data) // batch_size}, D Loss: {d_loss}, G Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate sample images and evaluate the GAN model\n",
    "\n",
    "# Create a function to generate and save sample images\n",
    "def generate_images(model, epoch, noise_dim, num_examples_to_generate, text_input, dir='./generated'):\n",
    "    \"\"\"\n",
    "    Generate a 4x4 grid of images from a GAN model.\n",
    "    Args:\n",
    "        model: The GAN model that will be used to generate the images.\n",
    "        epoch: The epoch at which the images will be generated.\n",
    "        noise_dim: The dimensionality of the noise vector that will be used to generate the images.\n",
    "        num_examples_to_generate: The number of images that will be generated.\n",
    "        text_input: The text input that will be used to condition the GAN model.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Process the text input with the dimension of noise\n",
    "    predictions = model([noise_dim, text_input])\n",
    "    # Create a new figure with a size of 3 x 3 inches\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    # Loop over the generated images\n",
    "    for i in range(predictions.shape[0]):\n",
    "        # Create a subplot in the figure\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        # Plot the image in the subplot\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        # Set the axis of the subplot to `off`\n",
    "        plt.axis('off')\n",
    "        # Save the figure as a PNG file with the name `image_at_epoch_{epoch}.png`\n",
    "        #path = os.path(dir)\n",
    "        plt.savefig(f'./{dir}/image_at_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Define the noise dimension for generating sample images\n",
    "noise_dim = 100\n",
    "\n",
    "# Create random noise and text input for generating sample images\n",
    "num_examples_to_generate = 16\n",
    "noise = np.random.normal(size=(num_examples_to_generate, noise_dim))\n",
    "text_input = get_random_text_input(0, num_examples_to_generate)  # Custom function to get text input\n",
    "\n",
    "# Generate and save sample images using the generator model\n",
    "generate_and_save_images(generator, 0, noise_dim, num_examples_to_generate, text_input)\n",
    "\n",
    "# Evaluate the GAN model\n",
    "# You can define evaluation metrics or procedures here to assess the quality of generated images and discriminator performance.\n",
    "# For example, you can use an external evaluation metric like FID (Fréchet Inception Distance) or manually inspect the generated images.\n",
    "\n",
    "# Save the trained generator and discriminator models\n",
    "generator.save('gan_generator.h5')\n",
    "discriminator.save('gan_discriminator.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Generation - Generate new images based on input text\n",
    "input_text = \"Hello, World!\"\n",
    "noise = np.random.normal(size=(1, latent_dim))\n",
    "text_input = preprocess_text(input_text)  # Custom function to preprocess input text\n",
    "generated_image = generator.predict([noise, text_input])\n",
    "\n",
    "# Display or save the generated image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-type Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "path = \"/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/GAN_VAE_Model\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128  # Dimensionality of the latent space\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# Variational Autoencoder\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "# instantiate VAE model and loss metrics\n",
    "vae = VAE(encoder, decoder)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "vae.compile(optimizer, loss=vae_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(dataset, epochs=10, batch_size=64)\n",
    "\n",
    "random_latent_vectors = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "generated_images = decoder(random_latent_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_model = '/content/scrabble-gan/res/out/big_ac_gan/model/generator_' + str(epochs)\n",
    "\n",
    "# number of samples to generate\n",
    "n_samples = 10\n",
    "# your sample string\n",
    "sample_string = 'machinelearning'\n",
    "\n",
    "# load trained model\n",
    "imported_model = tf.saved_model.load(path_to_saved_model)\n",
    "\n",
    "# inference loop\n",
    "for idx in range(1):\n",
    "  fake_labels = []\n",
    "  words = [sample_string] * 10\n",
    "  noise = tf.random.normal([n_samples, latent_dim])\n",
    "  \n",
    "  # encode words\n",
    "  for word in words:\n",
    "    fake_labels.append([char_vec.index(char) for char in word])\n",
    "  fake_labels = np.array(fake_labels, np.int32)\n",
    "\n",
    "  # run inference process\n",
    "  predictions = imported_model([noise, fake_labels], training=False)\n",
    "  # transform values into range [0, 1]\n",
    "  predictions = (predictions + 1) / 2.0\n",
    "\n",
    "  # plot results\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(10, 1, i + 1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    # plt.text(0, -1, \"\".join([char_vec[label] for label in fake_labels[i]]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with GAN_VAE_Model Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run shell\n",
    "# python ./GAN_VAE_Model/generate.py -d ./generated\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Project States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete\n"
     ]
    }
   ],
   "source": [
    "# Save Stuff\n",
    "\n",
    "# Put processed images in new folder\n",
    "import pickle\n",
    "with open(\"./filtered_data/processed_images.pkl\", \"wb\") as file:\n",
    "    pickle.dump(processed_images, file)\n",
    "print(\"Save Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete\n"
     ]
    }
   ],
   "source": [
    "# Load Stuff\n",
    "with open(\"./filtered_data/processed_images.pkl\", \"rb\") as file:\n",
    "    (\n",
    "        processed_images\n",
    "    ) = pickle.load(file)\n",
    "print(\"Load Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115239"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
