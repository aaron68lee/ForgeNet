{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# #import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'MOVE', 'to', 'stop', 'Mr.', 'Gaitskell', 'from', 'nominating', 'any', 'more', 'Labour', 'life', 'Peers', 'is', 'to', 'be', 'made', 'at', 'a', 'meeting', 'of', 'Labour', 'Ps', 'tomorrow', '.', 'Mr.', 'Michael', 'Foot', 'has', 'put', 'down', 'a', 'resolution', 'on', 'the', 'subject', 'and', 'he', 'is', 'to']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_last_word(line):\n",
    "  \"\"\"\n",
    "  Extracts the last word from a line \n",
    "  Input: String with space delimiter\n",
    "  Output: Word as a string\n",
    "  \"\"\"\n",
    "  words = line.split()\n",
    "  last_word = words[-1]\n",
    "  return last_word\n",
    "\n",
    "def extract_last_words(file_path, MAX_WORDS=np.inf):\n",
    "  \"\"\"\n",
    "  Extracts the last word from each line in the input file and returns the words as a list.\n",
    "  Input: File path, max number of words (OPTIONAL)\n",
    "  Output: List of last words\n",
    "  \"\"\"\n",
    "  \n",
    "  with open(file_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    last_words = []\n",
    "  \n",
    "  for line in lines:\n",
    "    if \"#\" in line:\n",
    "       continue\n",
    "\n",
    "    last_word = extract_last_word(line)\n",
    "    last_words.append(last_word)\n",
    "    if len(last_words) == MAX_WORDS:\n",
    "      break\n",
    "  return last_words\n",
    "\n",
    "words = extract_last_words(\"./Datasets/ascii/words.txt\", 40)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement functions to create these two dictionaries\n",
    "\n",
    "# fileToWord = {filepath: word as a string}\n",
    "# wordToFile = {word as a string: [filepath]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a01-000u-00-00', 'a01-000u-00-01', 'a01-000u-00-02', 'a01-000u-00-03', 'a01-000u-00-04', 'a01-000u-00-05', 'a01-000u-00-06', 'a01-000u-01-00', 'a01-000u-01-01', 'a01-000u-01-02', 'a01-000u-01-03', 'a01-000u-01-04', 'a01-000u-01-05', 'a01-000u-02-00', 'a01-000u-02-01', 'a01-000u-02-02', 'a01-000u-02-03', 'a01-000u-02-04', 'a01-000u-02-05', 'a01-000u-02-06', 'a01-000u-02-07', 'a01-000u-02-08', 'a01-000u-03-00', 'a01-000u-03-01', 'a01-000u-03-02', 'a01-000u-03-03', 'a01-000u-03-04', 'a01-000u-03-05', 'a01-000u-03-06', 'a01-000u-04-00', 'a01-000u-04-01', 'a01-000u-04-02', 'a01-000u-04-03', 'a01-000u-04-04', 'a01-000u-04-05', 'a01-000u-04-06', 'a01-000u-05-00', 'a01-000u-05-01', 'a01-000u-05-02', 'a01-000u-05-03']\n"
     ]
    }
   ],
   "source": [
    "def extract_filepaths(file_path, MAX_PATHS=float(\"inf\")):\n",
    "    \"\"\"\n",
    "    Takes the words.txt file and extracts all filepaths\n",
    "    Input: File of all filepaths\n",
    "    Output: List of unprocessed filepaths (processed with convert_filepath())\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        file_paths = []\n",
    "    \n",
    "        for line in lines:\n",
    "            if \"#\" in line:\n",
    "                continue\n",
    "            words = line.split()\n",
    "            current_file_path = words[0] \n",
    "            file_paths.append(current_file_path)\n",
    "            if len(file_paths) == MAX_PATHS:\n",
    "                break\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "paths = extract_filepaths(\"./Datasets/ascii/words.txt\", 40)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a01/a01-000u/a01-000u-00-00.png\n"
     ]
    }
   ],
   "source": [
    "def convert_filepath(filename):\n",
    "    \"\"\"\n",
    "    Converts filename (string) into proper filepath (string)\n",
    "    e.g. input: Filename = \"a1-00-121-000\"\n",
    "        output: filepath = \"a1/a1-00/a1-00-121-000.png\"\n",
    "    \"\"\"\n",
    "    parts = filename.split(\"-\")  # Split the filename by \"-\"\n",
    "    folder_parts = [parts[0]] + [f\"{parts[0]}-{parts[1]}\"]  # Exclude the last part (file name with extension)\n",
    "    \n",
    "    # Combine the folder parts using \"/\"\n",
    "    folder_path = \"/\".join(folder_parts)\n",
    "    \n",
    "    # Join the folder path with the original file name\n",
    "    filepath = f\"{folder_path}/{filename}.png\"\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "print(convert_filepath(\"a01-000u-00-00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a01/a01-000u/a01-000u-00-00.png': 'A', 'a01/a01-000u/a01-000u-00-01.png': 'MOVE', 'a01/a01-000u/a01-000u-00-02.png': 'to', 'a01/a01-000u/a01-000u-00-03.png': 'stop', 'a01/a01-000u/a01-000u-00-04.png': 'Mr.', 'a01/a01-000u/a01-000u-00-05.png': 'Gaitskell', 'a01/a01-000u/a01-000u-00-06.png': 'from', 'a01/a01-000u/a01-000u-01-00.png': 'nominating', 'a01/a01-000u/a01-000u-01-01.png': 'any', 'a01/a01-000u/a01-000u-01-02.png': 'more', 'a01/a01-000u/a01-000u-01-03.png': 'Labour', 'a01/a01-000u/a01-000u-01-04.png': 'life', 'a01/a01-000u/a01-000u-01-05.png': 'Peers', 'a01/a01-000u/a01-000u-02-00.png': 'is', 'a01/a01-000u/a01-000u-02-01.png': 'to', 'a01/a01-000u/a01-000u-02-02.png': 'be', 'a01/a01-000u/a01-000u-02-03.png': 'made', 'a01/a01-000u/a01-000u-02-04.png': 'at', 'a01/a01-000u/a01-000u-02-05.png': 'a', 'a01/a01-000u/a01-000u-02-06.png': 'meeting', 'a01/a01-000u/a01-000u-02-07.png': 'of', 'a01/a01-000u/a01-000u-02-08.png': 'Labour', 'a01/a01-000u/a01-000u-03-00.png': 'Ps', 'a01/a01-000u/a01-000u-03-01.png': 'tomorrow', 'a01/a01-000u/a01-000u-03-02.png': '.', 'a01/a01-000u/a01-000u-03-03.png': 'Mr.', 'a01/a01-000u/a01-000u-03-04.png': 'Michael', 'a01/a01-000u/a01-000u-03-05.png': 'Foot', 'a01/a01-000u/a01-000u-03-06.png': 'has', 'a01/a01-000u/a01-000u-04-00.png': 'put', 'a01/a01-000u/a01-000u-04-01.png': 'down', 'a01/a01-000u/a01-000u-04-02.png': 'a', 'a01/a01-000u/a01-000u-04-03.png': 'resolution', 'a01/a01-000u/a01-000u-04-04.png': 'on', 'a01/a01-000u/a01-000u-04-05.png': 'the', 'a01/a01-000u/a01-000u-04-06.png': 'subject', 'a01/a01-000u/a01-000u-05-00.png': 'and', 'a01/a01-000u/a01-000u-05-01.png': 'he', 'a01/a01-000u/a01-000u-05-02.png': 'is', 'a01/a01-000u/a01-000u-05-03.png': 'to'}\n",
      "{'A': ['a01/a01-000u/a01-000u-00-00.png'], 'MOVE': ['a01/a01-000u/a01-000u-00-01.png'], 'to': ['a01/a01-000u/a01-000u-00-02.png', 'a01/a01-000u/a01-000u-02-01.png', 'a01/a01-000u/a01-000u-05-03.png'], 'stop': ['a01/a01-000u/a01-000u-00-03.png'], 'Mr.': ['a01/a01-000u/a01-000u-00-04.png', 'a01/a01-000u/a01-000u-03-03.png'], 'Gaitskell': ['a01/a01-000u/a01-000u-00-05.png'], 'from': ['a01/a01-000u/a01-000u-00-06.png'], 'nominating': ['a01/a01-000u/a01-000u-01-00.png'], 'any': ['a01/a01-000u/a01-000u-01-01.png'], 'more': ['a01/a01-000u/a01-000u-01-02.png'], 'Labour': ['a01/a01-000u/a01-000u-01-03.png', 'a01/a01-000u/a01-000u-02-08.png'], 'life': ['a01/a01-000u/a01-000u-01-04.png'], 'Peers': ['a01/a01-000u/a01-000u-01-05.png'], 'is': ['a01/a01-000u/a01-000u-02-00.png', 'a01/a01-000u/a01-000u-05-02.png'], 'be': ['a01/a01-000u/a01-000u-02-02.png'], 'made': ['a01/a01-000u/a01-000u-02-03.png'], 'at': ['a01/a01-000u/a01-000u-02-04.png'], 'a': ['a01/a01-000u/a01-000u-02-05.png', 'a01/a01-000u/a01-000u-04-02.png'], 'meeting': ['a01/a01-000u/a01-000u-02-06.png'], 'of': ['a01/a01-000u/a01-000u-02-07.png'], 'Ps': ['a01/a01-000u/a01-000u-03-00.png'], 'tomorrow': ['a01/a01-000u/a01-000u-03-01.png'], '.': ['a01/a01-000u/a01-000u-03-02.png'], 'Michael': ['a01/a01-000u/a01-000u-03-04.png'], 'Foot': ['a01/a01-000u/a01-000u-03-05.png'], 'has': ['a01/a01-000u/a01-000u-03-06.png'], 'put': ['a01/a01-000u/a01-000u-04-00.png'], 'down': ['a01/a01-000u/a01-000u-04-01.png'], 'resolution': ['a01/a01-000u/a01-000u-04-03.png'], 'on': ['a01/a01-000u/a01-000u-04-04.png'], 'the': ['a01/a01-000u/a01-000u-04-05.png'], 'subject': ['a01/a01-000u/a01-000u-04-06.png'], 'and': ['a01/a01-000u/a01-000u-05-00.png'], 'he': ['a01/a01-000u/a01-000u-05-01.png']}\n"
     ]
    }
   ],
   "source": [
    "def extract_filepath_text_dic(file_path, MAX_WORDS=float(\"inf\")):\n",
    "    \"\"\"\n",
    "    Reads a file (like words.txt) and returns a dictionary mapping filepaths (.png) to words (strings)\n",
    "    Input: File\n",
    "    Output: Dictionary mapping filepaths to associated words\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    filepathToWord = {}\n",
    "  \n",
    "    for line in lines:\n",
    "        if \"#\" in line:\n",
    "            continue\n",
    "\n",
    "        text = extract_last_word(line)\n",
    "        filename = line.split()[0]\n",
    "        file_path = convert_filepath(filename)\n",
    "        filepathToWord[file_path] = text\n",
    "\n",
    "        if len(filepathToWord) == MAX_WORDS:\n",
    "            break\n",
    "    return filepathToWord\n",
    "\n",
    "filepaths_test = extract_filepath_text_dic(\"./Datasets/ascii/words.txt\", 40)\n",
    "print(filepaths_test)\n",
    "\n",
    "def generate_word_to_filepath(dictionary):\n",
    "    \"\"\"\n",
    "    Creates a word to filepath dictionary\n",
    "    Input: dictionary that maps filepaths to words\n",
    "    Output: dictionary that maps words to a list of filepaths\n",
    "    \"\"\"\n",
    "    wordToFilepath = {}\n",
    "\n",
    "    for filepath in dictionary: #loops by keys\n",
    "        word = dictionary[filepath]\n",
    "        if not word in wordToFilepath:\n",
    "            wordToFilepath[word] = [filepath]\n",
    "        else:\n",
    "            wordToFilepath[word].append(filepath)\n",
    "        \n",
    "    return wordToFilepath\n",
    "    \n",
    "words_test = generate_word_to_filepath(filepaths_test)\n",
    "print(words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb Cell 8\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./Datasets/ascii/words.txt\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# the actual file path\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m last_words_list \u001b[39m=\u001b[39m extract_text(file_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(last_words_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_text' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = \"./Datasets/ascii/words.txt\"  # the actual file path\n",
    "last_words_list = extract_text(file_path)\n",
    "\n",
    "print(last_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== DATA Extraction ===================================\n",
    "# ================== Iterate through all images in subfolders recursively =======================\n",
    "\n",
    "def extract_all(folder_path, MAX_ITER=float('inf')):\n",
    "  '''\n",
    "  DESC: for every image report in folder_path, generates a folder per patient with\n",
    "        OCR, RNFL, and text extractions in .jpg format\n",
    "  INPUT: folder_path | path to folder with images of reports\n",
    "  RETURNS: none\n",
    "  '''\n",
    "  # Get a list of files in the folder\n",
    "  file_list = os.listdir(folder_path)\n",
    "\n",
    "  # Initialize tqdm with the total number of files\n",
    "  progress_bar = tqdm(file_list, desc='Processing Images', unit='image')\n",
    "  iter = 0\n",
    "\n",
    "  for file_name in progress_bar:\n",
    "    print(\"Iteration: \", iter, \" | File:\", file_name)\n",
    "    if iter >= MAX_ITER:\n",
    "      break\n",
    "\n",
    "    # Check if the file has an image extension\n",
    "    if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        img = os.path.splitext(file_name)[0]  # Get the image name without extension\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        output_folder = os.path.join(folder_path, img)\n",
    "\n",
    "        if not os.path.exists(output_folder): # folder for patient i with extracted data\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        extract_rnfl(image_path, output_path=output_folder + '/')\n",
    "        extract_oct(image_path, output_path=output_folder + '/')\n",
    "        extract_textbox(image_path, output_path=output_folder + '/')\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "  print(\"Extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Project States\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all variables and data structures to a file\n",
    "with open(\"./variables.pkl\", \"wb\") as file:\n",
    "    vars = (\n",
    "            # list variable names here\n",
    "           \n",
    "            )\n",
    "    pickle.dump(vars, file)\n",
    "print(\"Save Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all variables and data structures from a file\n",
    "with open(\"./variables.pkl\", \"rb\") as file:\n",
    "    (\n",
    "       # list var names here\n",
    "    ) = pickle.load(file)\n",
    "print(\"Load Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
