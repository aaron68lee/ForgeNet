{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "\n",
    "MAX_PRINT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'MOVE', 'to', 'stop', 'Mr.', 'Gaitskell', 'from', 'nominating', 'any', 'more']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_last_word(line):\n",
    "  \"\"\"\n",
    "  Extracts the last word from a line \n",
    "  Input: String with space delimiter\n",
    "  Output: Word as a string\n",
    "  \"\"\"\n",
    "  words = line.split()\n",
    "  last_word = words[-1]\n",
    "  return last_word\n",
    "\n",
    "def extract_last_words(file_path, MAX_WORDS=np.inf):\n",
    "  \"\"\"\n",
    "  Extracts the last word from each line in the input file and returns the words as a list.\n",
    "  Input: File path, max number of words (OPTIONAL)\n",
    "  Output: List of last words\n",
    "  \"\"\"\n",
    "  \n",
    "  with open(file_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    last_words = []\n",
    "  \n",
    "  for line in lines:\n",
    "    if \"#\" in line:\n",
    "       continue\n",
    "\n",
    "    last_word = extract_last_word(line)\n",
    "    last_words.append(last_word)\n",
    "    if len(last_words) == MAX_WORDS:\n",
    "      break\n",
    "  return last_words\n",
    "\n",
    "words = extract_last_words(\"./Datasets/ascii/words.txt\", 40)\n",
    "print(words[0:MAX_PRINT])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a01-000u-00-00', 'a01-000u-00-01', 'a01-000u-00-02', 'a01-000u-00-03', 'a01-000u-00-04', 'a01-000u-00-05', 'a01-000u-00-06', 'a01-000u-01-00', 'a01-000u-01-01', 'a01-000u-01-02']\n",
      "['a01-000u-00', 'a01-000u-01', 'a01-000u-02', 'a01-000u-03', 'a01-000u-04', 'a01-000u-05', 'a01-000u-06', 'a01-000x-00', 'a01-000x-01', 'a01-000x-02']\n"
     ]
    }
   ],
   "source": [
    "def extract_filepaths(file_path, MAX_PATHS=float(\"inf\")):\n",
    "    \"\"\"\n",
    "    Takes the words.txt file and extracts all filepaths\n",
    "    Input: File of all filepaths\n",
    "    Output: List of unprocessed filepaths (processed with convert_filepath())\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        file_paths = []\n",
    "    \n",
    "        for line in lines:\n",
    "            if \"#\" in line:\n",
    "                continue\n",
    "            words = line.split()\n",
    "            current_file_path = words[0] \n",
    "            file_paths.append(current_file_path)\n",
    "            if len(file_paths) == MAX_PATHS:\n",
    "                break\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "paths = extract_filepaths(\"./Datasets/ascii/words.txt\", 40)\n",
    "print(paths[0:MAX_PRINT])\n",
    "paths = extract_filepaths(\"./Datasets/ascii/lines.txt\", 40)\n",
    "print(paths[0:MAX_PRINT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a01/a01-000u/a01-000u-00-00.png\n"
     ]
    }
   ],
   "source": [
    "def convert_filepath(filename):\n",
    "    \"\"\"\n",
    "    Converts filename (string) into proper filepath (string)\n",
    "    e.g. input: Filename = \"a1-00-121-000\"\n",
    "        output: filepath = \"a1/a1-00/a1-00-121-000.png\"\n",
    "        input: Filename = \"a1-00-121\"\n",
    "        output: filepath = \"a1/a1-00/a1-00-121.png\"\n",
    "    \"\"\"\n",
    "    parts = filename.split(\"-\")  # Split the filename by \"-\"\n",
    "    folder_parts = [parts[0]] + [f\"{parts[0]}-{parts[1]}\"]  # Exclude the last part (file name with extension)\n",
    "    \n",
    "    # Combine the folder parts using \"/\"\n",
    "    folder_path = \"/\".join(folder_parts)\n",
    "    \n",
    "    \n",
    "    # Join the folder path with the original file name\n",
    "    filepath = f\"{folder_path}/{filename}.png\"\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "print(convert_filepath(\"a01-000u-00-00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'printDic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m filepathToWord\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m filepaths_test \u001b[39m=\u001b[39m extract_filepath_text_dic(\u001b[39m\"\u001b[39m\u001b[39m./Datasets/ascii/words.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m40\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m printDic(filepaths_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m filepaths_test \u001b[39m=\u001b[39m extract_filepath_text_dic(\u001b[39m\"\u001b[39m\u001b[39m./Datasets/ascii/sentences.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m40\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aaron68lee/Documents/Coding-Projects/ForgeNet/dataloader.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m printDic(filepaths_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'printDic' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_filepath_text_dic(file_path, MAX_WORDS=float(\"inf\")):\n",
    "    \"\"\"\n",
    "    Reads a file (like words.txt) and returns a dictionary mapping filepaths (.png) to words (strings)\n",
    "    Input: File\n",
    "    Output: Dictionary mapping filepaths to associated words\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    filepathToWord = {}\n",
    "  \n",
    "    for line in lines:\n",
    "        if \"#\" in line:\n",
    "            continue\n",
    "\n",
    "        text = extract_last_word(line)\n",
    "        filename = line.split()[0]\n",
    "        file_path = convert_filepath(filename)\n",
    "        filepathToWord[file_path] = text\n",
    "\n",
    "        if len(filepathToWord) == MAX_WORDS:\n",
    "            break\n",
    "    return filepathToWord\n",
    "\n",
    "filepaths_test = extract_filepath_text_dic(\"./Datasets/ascii/words.txt\", 40)\n",
    "printDic(filepaths_test)\n",
    "filepaths_test = extract_filepath_text_dic(\"./Datasets/ascii/sentences.txt\", 40)\n",
    "printDic(filepaths_test)\n",
    "\n",
    "def generate_word_to_filepath(dictionary):\n",
    "    \"\"\"\n",
    "    Creates a word to filepath dictionary\n",
    "    Input: dictionary that maps filepaths to words\n",
    "    Output: dictionary that maps words to a list of filepaths\n",
    "    \"\"\"\n",
    "    wordToFilepath = {}\n",
    "\n",
    "    for filepath in dictionary: #loops by keys\n",
    "        word = dictionary[filepath]\n",
    "        if not word in wordToFilepath:\n",
    "            wordToFilepath[word] = [filepath]\n",
    "        else:\n",
    "            wordToFilepath[word].append(filepath)\n",
    "        \n",
    "    return wordToFilepath\n",
    "# words_test = generate_word_to_filepath(filepaths_test)\n",
    "# printDic(words_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== DATA Extraction ===================================\n",
    "# ================== Iterate through all images in subfolders recursively =======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_all(folder_path, MAX_ITER=float('inf')):\n",
    "  '''\n",
    "  DESC: for every image report in folder_path, generates a folder per patient with\n",
    "        OCR, RNFL, and text extractions in .jpg format\n",
    "  INPUT: folder_path | path to folder with images of reports\n",
    "  RETURNS: none\n",
    "  '''\n",
    "  # Get a list of files in the folder\n",
    "  file_list = os.listdir(folder_path)\n",
    "\n",
    "  # Initialize tqdm with the total number of files\n",
    "  progress_bar = tqdm(file_list, desc='Processing Images', unit='image')\n",
    "  iter = 0\n",
    "\n",
    "  for file_name in progress_bar:\n",
    "    print(\"Iteration: \", iter, \" | File:\", file_name)\n",
    "    if iter >= MAX_ITER:\n",
    "      break\n",
    "\n",
    "    # Check if the file has an image extension\n",
    "    if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        img = os.path.splitext(file_name)[0]  # Get the image name without extension\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        output_folder = os.path.join(folder_path, img)\n",
    "\n",
    "        if not os.path.exists(output_folder): # folder for patient i with extracted data\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        # do processing\n",
    "        '''\n",
    "        extract_rnfl(image_path, output_path=output_folder + '/')\n",
    "        extract_oct(image_path, output_path=output_folder + '/')\n",
    "        extract_textbox(image_path, output_path=output_folder + '/')\n",
    "        '''\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "  print(\"Extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print formatted dictionary\n",
    "\n",
    "def printDic(dic, MAX_PRINT=10):\n",
    "    for i, (key, value) in enumerate(dic.items()):\n",
    "        if i > MAX_PRINT:\n",
    "            break\n",
    "        print(key, value)\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Project States\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Complete\n"
     ]
    }
   ],
   "source": [
    "filepaths_lst = extract_filepaths(\"./Datasets/ascii/words.txt\")\n",
    "words_lst = extract_last_words(\"./Datasets/ascii/words.txt\")\n",
    "filepaths_dic = extract_filepath_text_dic(\"./Datasets/ascii/words.txt\")\n",
    "words_dic = generate_word_to_filepath(filepaths_dic)\n",
    "# Save all variables and data structures to a file\n",
    "with open(\"./variables.pkl\", \"wb\") as file:\n",
    "    vars =  (\n",
    "            # list variable names here\n",
    "            filepaths_lst,\n",
    "            words_lst,\n",
    "            filepaths_dic,\n",
    "            words_dic\n",
    "            )\n",
    "    pickle.dump(vars, file)\n",
    "print(\"Save Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete\n"
     ]
    }
   ],
   "source": [
    "# Load all variables and data structures from a file\n",
    "with open(\"./variables.pkl\", \"rb\") as file:\n",
    "    (\n",
    "       filepaths_lst,\n",
    "            words_lst,\n",
    "            filepaths_dic,\n",
    "            words_dic\n",
    "    ) = pickle.load(file)\n",
    "print(\"Load Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test what we loaded was correct: print first 5 in words_dic\n",
    "#test = [print(key, value) for i, (key, value) in enumerate(words_dic.items()) if i < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
